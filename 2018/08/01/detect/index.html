
 <!DOCTYPE HTML>
<html >
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>目标检测篇 | LI jianan&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="LI jianan">
    
    <meta name="description" content="(这是一个开头)                                                                                                                                              ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.jpg">
    

  
  

    <link rel="stylesheet" href="/css/style.css">
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="LI jianan&#39;s Blog">LI jianan&#39;s Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/08/01/detect/" title="目标检测篇" itemprop="url">目标检测篇</a>
  </h1>
  <p class="article-time">
    <time datetime="2018-08-01T01:12:46.000Z" itemprop="datePublished">2018-08-01</time>
  </p>
</header>
	<div class="article-content">
		
		
		<p>(这是一个开头)                                                                                                                                                                                                                                                                                                                                                                              </p>
<h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><h2 id="Anchor-boxes"><a href="#Anchor-boxes" class="headerlink" title="Anchor boxes"></a>Anchor boxes</h2><p>Anchor boxes的尺寸-&gt;三个面积尺寸：128^2，256^2，512^2，然后在每个尺寸下，取三种不同的长宽比例：1:1、1:2、2:1，这样一来，我们得到9种面积尺寸的anchor。示意图如下：<br><img src="/2018/08/01/detect/anchor.png" alt=""><br>下面是整个Faster R-CNN结构的示意图<br><img src="/2018/08/01/detect/frcnn2.png" alt=""><br><img src="/2018/08/01/detect/frcnn.png" alt=""><br>利用anchor是从第二列这个位置开始进行处理，这时候图片已经经过一系列卷积层和池化层以及ReLU，得到了这里的feature：51×39×256。<br>在这个特征图上，通过一个3×3的滑动窗口，在这个51×39的区域上进行滑动，stride=1，padding=2，这样一来，得到51×39个3×3的窗口。<br>对于每个3×3的窗口，作者假定它来自9种不同原图区域的池化，并且这些池化在原始图片中的中心点都一样。这样一来，在每个窗口位置，我们都可以根据9种anchor，逆向推导出它所对应原图中的一片区域，区域的尺寸以及坐标都是已知的。<em>而这些区域，就是我们想要的proposal，所以我们通过滑动窗口和anchors，成功得到了51×39×9(约20K)个原始图片的proposal。</em>接下来，每个proposal输出6个参数。</p>
<h2 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h2><p><img src="/2018/08/01/detect/ye.jpg" alt=""><br>可以看到在Conv1中的每一个单元所能看到的原始图像范围是3×3，而由于Conv2的每个单元都是由 2×2 范围的Conv1构成，因此回溯到原始图像，其实是能够看到 5×5 的原始图像范围的。因此我们说Conv1的感受野是3，Conv2的感受野是5. 输入图像的每个单元的感受野被定义为1，这应该很好理解，因为每个像素只能看到自己。<br>由于图像是二维的，具有空间信息，因此感受野的实质其实也是一个二维区域。但业界通常将感受野定义为一个正方形区域，因此也就使用边长来描述其大小了。在接下来的讨论中，本文也只考虑宽度一个方向。<br>接下来我们使用一种并不常见的方式来展示CNN的层与层之间的关系(如下图，请将脑袋向左倒45°观看)，并且配上我们对原图像的编号。<br><img src="/2018/08/01/detect/ye2.jpg" alt=""><br>我们用 r<sub>n</sub> 来表示第 n 个卷积层中，每个单元的感受野（即数字序列的长度）；蓝色的部分表示卷积操作，用 k<sub>n</sub> 和 s<sub>n</sub> 分别表示第 n 个卷积层的kernel_size和stride。<br>看如何得到r<sub>2</sub>这个过程，r<sub>2</sub>能获得的最大感受野是r<sub>1</sub>×k<sub>2</sub> = 3×2 = 6，但这里存在重叠，重叠部分一共有k<sub>2</sub> - 1个，重叠里面有多长是r<sub>1</sub> - s<sub>1</sub>。<br>r<sub>2</sub> = r<sub>1</sub>×k<sub>2</sub> - (r<sub>1</sub> - s<sub>1</sub>)×(k<sub>2</sub> - 1)<br>推理过程没有完全理解，直接放上最后结论：<br><img src="/2018/08/01/detect/ye3.svg" alt=""><br>整理，<br><img src="/2018/08/01/detect/ye4.svg" alt=""><br>–感受野<br>proposal做的事情是它已知图像中某一部分的feature，判断anchor(还是bounding box？)是物体的概率。<strong>如果anchor比感受野大，就相当于只看到我关心区域(anchor)的一部分，通过部分来判断整体；如果anchor比感受野小，就相当于得到了比我关心区域更大的区域的信息，判断我关心区域是不是物体。</strong></p>
<p>基于前面的理解再继续来看这篇论文。<br><strong>3.1 Region Proposal Networks</strong><br>配合前面Faster RCNN的结构示意图来看下面这个RPN的结构示意图：<br><img src="/2018/08/01/detect/rpn.PNG" alt=""></p>
<p><strong>白话RPN干的事情就是在conv layers输出map的每一个像素点周围做一次3×3的卷积(通过直接在map上做3×3的卷积)，输出一个1×1×256的向量分别又做一次1×1的卷积，分别输出1×1×(9×2)和1×1×(9×4)的RPN网络预测结果。</strong></p>
<p><strong>3.1.1 Anchors</strong><br>再加入Anchors的概念，就像上文提到的，作者在每一个sliding-window location，同时预测多个区域提案(也就是9个)。</p>
<p><strong>3.1.2 Loss Function</strong><br><strong>为了训练RPN网络，作者给每个Anchors打上标签(这是一个object或者不是)。</strong>打标签的方法这里不再赘述。<br>loss函数如下图，各变量含义是<strong>i是一个mini-batch中Anchors的索引，pi是Anchors i被预测是一个object的概率，pi星是Anchors i的ground truth。ti是跟Anchors i对应的四个坐标位置预测出来的bounding box，ti星是跟Anchors i对应的ground truth box的坐标位置。</strong><br><img src="/2018/08/01/detect/loss.PNG" alt=""><br><strong>Lcls是softmax loss，</strong><br><img src="/2018/08/01/detect/loss1.PNG" alt=""><br><strong>g是smooth L<sub>1</sub> loss，对异常值不敏感：</strong><br><img src="/2018/08/01/detect/loss2.PNG" alt=""></p>
<p><strong>3.2 Sharing Features for RPN and Fast R-CNN</strong></p>
<ol>
<li><strong>按照3.1.3描述的训练RPN网络，这个网络的参数用ImageNet-pre-trained结果初始化，然后根据region proposal任务微调。</strong></li>
<li><strong>单独训练Fast RCNN网络用第1步得到的proposals，这个检测网络参数也是用ImageNet-pre-trained结果初始化。</strong></li>
</ol>
<p>目前为止，还没有共享参数</p>
<ol start="3">
<li><strong>用检测网络的参数初始化RPN的训练网络，然后固定共享卷积网络参数，微调RPN独有的部分(那就一个conv+relu了，看起来有点少？)</strong></li>
</ol>
<p>现在两个网络共享参数了</p>
<ol start="4">
<li><strong>共享卷积网络参数还是固定，微调Fast RCNN独有的网络部分。</strong></li>
</ol>
<p><strong>Fast R-CNN 2.1. The RoI pooling layer</strong><br><strong>把任意大小的输入得到H×W大小的输出，每一个RoI由(r, c, h, w)定义，(r, c)是ROI左上角坐标，(h, w)是输入的高和宽。RoI Pooling的方法是先把输入划分成H×W个h/H、w/W的子窗口，在每一个子窗口做max-pooling得到一个值，最后输出的大小自然就固定成H×W。</strong></p>
<h1 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h1><p><strong>mean Average Precision</strong><br>mAP是在目标检测问题中最常用的衡量标准，专门用于预测目标位置并分类的算法，比如定位模型，目标检测模型，和分割模型。<br><img src="/2018/08/01/detect/map-1.png" alt=""></p>
<h2 id="Why-mAP"><a href="#Why-mAP" class="headerlink" title="Why mAP?"></a>Why mAP?</h2><p>在目标检测问题中，每张图片都可能会有不同classes的不同objects。分类模型和定位模型都需要被评估。因此，在图像分类问题中使用的精度标准不能直接用在检测问题中。</p>
<h2 id="关于Ground-Truth"><a href="#关于Ground-Truth" class="headerlink" title="关于Ground Truth"></a>关于Ground Truth</h2><p>对于任何算法度量都是根据Ground Truth进行评估，我们只知道Training、Validation和Test的Ground Truth。对于目标检测，Ground Truth包括图像和图像中每一个object的类别和bounding box。<br><img src="/2018/08/01/detect/map-2.png" alt=""><br>给计算机的是实际的图像(jpg, png等)和注释文本(bounding box的坐标x, y, width, height和类别)，上面的图片只是为了我们人可视化。<br>所以，对于这张特定的图片，我们的模型在训练的时候得到的数据如下<br><img src="/2018/08/01/detect/map-3.png" alt=""><br>以及3组定义了Ground Truth的数<br><img src="/2018/08/01/detect/map-3-1.png" alt=""></p>
<h2 id="计算mAP"><a href="#计算mAP" class="headerlink" title="计算mAP"></a>计算mAP</h2><p>模型会返回很多预测，但是在这些预测中，大多数预测的置信度都很低，我们只考虑在某个数值置信度之上的预测。<br>下面是我们模型预测的结果：<br><img src="/2018/08/01/detect/map-4.png" alt=""><br>我们首先需要判断这些检测的正确性，度量标准是IoU。<br>为了计算Precision和Recall，我们必须定义<strong>True Positives, False Positives, True Negatives和False Negatives</strong>。<br>我们使用IoU来得到True Positives和False Positives，我们现在需要判断预测结果(Positives)到底是TP还是FP，方法就是和Ground Truth计算IoU，如果IoU &gt; 0.5，那就是True Positives；IoU ≤ 0.5，就是False Positives。COCO数据库建议使用多个阈值，PASCAL VOC数据库使用的就是0.5标准。<br>计算True Negatives是没什么用的，所以我们只计算False Negatives，即预测结果是Negatives，但IoU &gt; 0.5。<br>通过改变我们的置信阈值，我们可以改变预测框是正的还是负的。所有达到预测的预测(Bounding box + Class)都被认为是Positives，低于阈值的预测都是Negatives。<br>再一次说明，对于每一张图片，我们已经有Ground Truth告诉我们在这张图片中每一个种类的object的数量。<br>好了，接下来<br><strong>Precision = TP/(TP+FP)</strong>在预测是正例的里面，真的是正例的比例<br><strong>Recall = TP/(TP+FN)</strong>在Ground Truth是True的object里面，被预测成Positives的比例<br>正如前面提到的，我们至少还有另外两个变量决定了Precision和Recall，它们是IoU的阈值和置信度阈值。</p>
<p>在the PASCAL Visual Objects Classes(VOC) challenge(IoU &gt; 0.5)，定义mAP的计算方法：<br>我们选择11种不同的置信度，使得Recall值计算出来等于0, 0.1, 0.2, …, 0.9和1.0。AP定义成11个Recall值对应的Precision的平均值。<br><strong>Basically we use the maximum precision for a given recall value.？</strong><br>mAP也就是在每一个种类按照上述方法得到的AP的平均。<br>关于mAP注意以下三点：</p>
<ol>
<li>mAP总是根据一个数据集得到的。</li>
<li>虽然很难解释模型输出的绝对量化，但mAP帮助我们通过得到了一个相当好的相对度量。当我们在流行的公共数据集上计算这个度量时，这个度量可以很容易地用于比较对象检测的新旧方法。</li>
<li>根据训练数据中类的分布情况，AP可能从某些类(具有良好的训练数据)非常高到非常低(具有较少/坏数据的类)。你的mAP可能是稳健的，但你的模型可能对某些类很好，对某些类很坏。因此，在分析模型结果时，最好查看单个类的AP。<strong>这些值也可以作为增加更多训练样本的指标。</strong></li>
</ol>
<h1 id="top5"><a href="#top5" class="headerlink" title="top5"></a>top5</h1><p>top1就是你预测的label取最后概率向量里面最大的那一个作为预测结果，你的预测结果中概率最大的那个类必须是正确类别才算预测正确。而top5就是最后概率向量最大的前五名中出现了正确概率即为预测正确。</p>
<h1 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h1><p>YOLO的CNN网络将输入的图片分割成S×S网络，然后每个单元格负责去检测那些中心点落在该格子内的目标，如图所示，可以看到狗这个目标的中心落在左下角的一个单元格内，那么该单元负责预测这个狗。每个单元格会预测B个边界框以及边界框的置信度(confidence score)。所谓置信度其实包含两个方面，<strong>一是边界框可能含有目标的可能性大小，而是这个边界框的准确度。</strong>前者记为<strong>Pr(Object)</strong>，当<br><img src="/2018/08/01/detect/yolo2.jpg" alt=""></p>
<ul>
<li>每个grid有30维，这30维中，8维是回归box的坐标，2维是box的confidence，还有20维是类别。 其中坐标的x,y用对应网格的offset归一化到0-1之间，w,h用图像的width和height归一化到0-1之间。</li>
<li>在实现中，最主要的就是怎么设计损失函数，让这个三个方面得到很好的平衡。作者简单粗暴的全部采用了<strong>sum-squared error loss</strong>来做这件事。<br>这种做法存在以下几个问题：<br>第一，8维的localization error和20维的classification error同等重要显然是不合理的；<br>第二，如果一个网格中没有object（一幅图中这种网格很多），那么就会将这些网格中的box的confidence push到0，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。<br>解决办法：<br>更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为loss weight在pascal VOC训练中取5。<br>对没有object的box的confidence loss，赋予小的loss weight，loss weight在pascal VOC训练中取0.5。<br>有object的box的confidence loss和类别的loss的loss weight正常取1。</li>
<li>对不同大小的box预测中，相比于大box预测偏一点，小box预测偏一点肯定更不能被忍受的。而sum-square error loss中对同样的偏移loss是一样。<br>为了缓和这个问题，作者用了一个比较取巧的办法，就是将box的width和height取平方根代替原本的height和width。这个参考下面的图很容易理解，小box的横轴值较小，发生偏移时，反应到y轴上相比大box要大。<br><img src="/2018/08/01/detect/yolo1.png" alt=""></li>
<li>一个网格预测多个box，希望的是每个box predictor专门负责预测某个object。具体做法就是看当前预测的box与ground truth box中哪个IoU大，就负责哪个。这种做法称作box predictor的specialization。</li>
<li>最后整个的损失函数如下所示：<br><img src="/2018/08/01/detect/yolo.png" alt=""></li>
</ul>
<p>YOLO的缺点：</p>
<ul>
<li>YOLO对相互靠的很近的物体，还有很小的群体检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类。</li>
<li>出现新的对象框大小时难以适应。</li>
<li>大框的定位误差一般影响较小，但是小框的定位误差会极大得影响IoU，YOLO的主要问题在于不正确定位。</li>
</ul>
<h1 id="YOLO9000"><a href="#YOLO9000" class="headerlink" title="YOLO9000"></a>YOLO9000</h1><p>YOLOv1虽然检测速度很快，但是在检测精度上不如R-CNN系列检测方法，YOLOV1在在物体定位方面（localization）不够准确，并且召回率（recall）较低。YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。YOLOv2的改进策略如图所示，可以看出，大部分的改进方法都可以比较显著提升模型的mAP。下面详细介绍各个改进策略。<br><img src="/2018/08/01/detect/yolo2-0.png" alt=""></p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</a><br>具有以下几点特性</p>
<ul>
<li>快速收敛，你可以选择更大的初始学习率</li>
<li>提高网络泛化能力，可以不考虑dropout和L2正则项系数或者使用更小的L2正则项系数</li>
<li>不需要AlexNet中用到的局部响应归一化，BN本身是一个归一化网络层</li>
<li>可以把训练数据彻底打乱</li>
</ul>
<h3 id="为什么需要BN"><a href="#为什么需要BN" class="headerlink" title="为什么需要BN"></a>为什么需要BN</h3><ol>
<li><strong>神经网络的本质是为了学习数据分布，一旦训练数据与测试数据的分布不同，网络的泛化能力也就会降低。</strong></li>
<li><strong>一旦每次批训练的分布不同，那么网络在每次迭代都要去学习适应不同的分布，这样将会大大降低网络的训练速度，这也是我们为什么需要BN的原因。</strong></li>
</ol>
<p>网络在前面几层发生的微小变化到后面层都会被累积放大，如果不做BN，那么训练数据的分布一直在发生变化，将会影响网络的学习速度。<br>我们以第二个层为例：网络第二层输入是由第一层参数和输入得到的，而第一层的参数在整个训练过程中一直在发生变化，因此必然会引起后面每一个层输入数据分布的改变。我们把网络中间层在训练过程中分布的改变称之为<strong>Internal Covariate Shift</strong>。BN则是为了解决中间层数据分布一直发生变化的问题。</p>
<p>在网络的每一层输入之前，插入一个归一化层，然后再进入下一层网络。<br>其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，这可怎么办？于是文献使出了一招惊天地泣鬼神的招式：<strong>变换重构</strong>，引入了可学习参数γ、β，这就是算法关键之处：<br><img src="/2018/08/01/detect/yolo2-1.png" alt=""><br>每一层神经元都有这样的参数：<br><img src="/2018/08/01/detect/yolo2-2.png" alt=""><br>让我们的网络可以学习恢复出原始网络所要学习的特征分布。<br>最后Batch Normalization网络层的前向传导过程公式就是：<br><img src="/2018/08/01/detect/yolo2-3.png" alt=""><br>在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。</p>
<h2 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h2><p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为 224×224 的图片作为输入，分辨率相对较低，不利于检测模型。所以YOLOv1在采用 224×224 分类模型预训练后，将分辨率增加至 448×448 ，并使用这个高分辨率在检测数据集上finetune。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。所以YOLOv2增加了在ImageNet数据集上使用 448×448 输入来finetune分类网络这一中间过程（10 epochs），这可以使得模型在检测数据集上finetune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p>
<h2 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h2><p>YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。为了使检测所用的特征图分辨率更高，移除其中的一个pool层。在检测模型中，YOLOv2不是采用 448×448 图片作为输入，而是采用 416×416 大小。因为YOLOv2模型下采样的总步长为 32 ，对于 416×416 大小的图片，最终得到的特征图大小为 13×13 ，维度是奇数，这样特征图恰好只有一个中心位置。<br>对于YOLOv1，每个cell都预测2个boxes，每个boxes包含5个值： (x, y, w, h, c) ，前4个值是边界框位置与大小，最后一个值是置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。YOLOv2使用了anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值。<br>使用anchor boxes之后，YOLOv2的mAP有稍微下降，召回率大大提升，由原来的81%升至88%。</p>
<h2 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h2><p>pass</p>
<h2 id="New-Network-Darknet-19"><a href="#New-Network-Darknet-19" class="headerlink" title="New Network: Darknet-19"></a>New Network: Darknet-19</h2><p>YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图4所示。Darknet-19与VGG16模型设计原则是一致的，主要采用 3×3 卷积，采用 2×2 的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在 3×3 卷积之间使用 1×1 卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。</p>
<p>## </p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/目标检测/">目标检测</a>
  </div>




<div class="article-share" id="share">

  <div data-url="https://www.lijn.tech/2018/08/01/detect/" data-title="目标检测篇 | LI jianan&#39;s Blog" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/08/02/algo/" title="为数学建模准备的算法">
  <strong>Previous:</strong><br/>
  <span>
  为数学建模准备的算法</span>
</a>
</div>


<div class="next">
<a href="/2018/07/31/leetcode/"  title="刷LeetCode">
 <strong>Next:</strong><br/> 
 <span>刷LeetCode
</span>
</a>
</div>

</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span> If you are interested in Computer Vision, you can contact </span>
			<span> <a href="https://github.com/JnanLi">me.</a> </span>
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>




<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  </body>
</html>
