
 <!DOCTYPE HTML>
<html >
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral | LI jianan&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="LI jianan">
    
    <meta name="description" content="本文采用无监督学习的方法来估计深度，本文能实现在35ms内恢复一张图512×256的图只需要25ms（GPU）。                                                                                              ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.jpg">
    

  
  

    <link rel="stylesheet" href="/css/style.css">
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="LI jianan&#39;s Blog">LI jianan&#39;s Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/06/19/monodepth/" title="Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral" itemprop="url">Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral</a>
  </h1>
  <p class="article-time">
    <time datetime="2018-06-19T07:20:52.000Z" itemprop="datePublished">2018-06-19</time>
  </p>
</header>
	<div class="article-content">
		
		
		<p>本文采用无监督学习的方法来估计深度，本文能实现在35ms内恢复一张图512×256的图只需要25ms（GPU）。                                                                                                                                                                                                                                                                                   </p>
<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h1><p>基于极几何约束，我们通过减少图像重构损失来训练网络以生成视差图像。论文那三个损失函数合在一起作为最终的损失函数。<br><a href="https://ieeexplore.ieee.org/document/8100182/" target="_blank" rel="noopener">Unsupervised Monocular Depth Estimation with Left-Right Consistency</a></p>
<h1 id="2-网络结构"><a href="#2-网络结构" class="headerlink" title="2. 网络结构"></a>2. 网络结构</h1><p>本文基于的直觉是：给定校准之后的双目相机，如果我们能学习一个函数，输入一张左视图，重构出对应的右视图，那就说明我们学习到了拍摄场景的3D形状。下图展现了最近在无监督方向上方法的演变：<br><img src="/2018/06/19/monodepth/fig3.PNG" alt=""><br>网络结构由两个部分组成：一个编码器(从conv1到conv7b)和一个解码器(从upconv7开始)，网络参数如下表。解码器与编码器之间<strong>skip connections</strong>，这么做是为了学习到细节部分。<br><img src="/2018/06/19/monodepth/tab1.PNG" alt=""></p>
<h1 id="3-训练损失函数"><a href="#3-训练损失函数" class="headerlink" title="3. 训练损失函数"></a>3. 训练损失函数</h1><p>作者使用金字塔输入，即一张输入图片，通过金字塔网络得到<strong>[I, I/2, I/4, I/8]</strong>，在<strong>python</strong>里这就是一个<strong>list</strong>，用<strong>for</strong>语句可以很方便的同时计算4个尺寸的训练损失函数。<br>作者把他提出来的三种损失函数分别称为<strong>Appearance Matching Loss[目的是预测得到的左视图和输入的左视图看起来像], Disparity Smoothness Loss[目的是得到的视差图平滑], and Left-Right Disparity Consistency Loss[预测出来的右视差图根据预测出来的左视差图得到新的左视差图，这个新的左视差图要等于预测出来的左视差图]</strong>。<br><img src="/2018/06/19/monodepth/lossall.png" alt="lossall"></p>
<h2 id="3-1-Appearance-Matching-Loss"><a href="#3-1-Appearance-Matching-Loss" class="headerlink" title="3.1 Appearance Matching Loss"></a>3.1 Appearance Matching Loss</h2><p><img src="/2018/06/19/monodepth/ap.png" alt="ap"><br><a href="https://ieeexplore.ieee.org/document/1284395/" target="_blank" rel="noopener">Image quality assessment: from error visibility to structural similarity</a><br>下面具体的实现和上面论文中描述的基本相符，<strong>SSIM</strong>是用来衡量两幅图的结构相似性的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SSIM</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">    C1 = <span class="number">0.01</span>**<span class="number">2</span></span><br><span class="line">    C2 = <span class="number">0.03</span>**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    mu_x = slim.avg_pool2d(x, <span class="number">3</span>, <span class="number">1</span>, <span class="string">'VALID'</span>)</span><br><span class="line">    mu_y = slim.avg_pool2d(y, <span class="number">3</span>, <span class="number">1</span>, <span class="string">'VALID'</span>)</span><br><span class="line"></span><br><span class="line">    sigma_x  = slim.avg_pool2d(x ** <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">'VALID'</span>) - mu_x ** <span class="number">2</span></span><br><span class="line">    sigma_y  = slim.avg_pool2d(y ** <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">'VALID'</span>) - mu_y ** <span class="number">2</span></span><br><span class="line">    sigma_xy = slim.avg_pool2d(x * y , <span class="number">3</span>, <span class="number">1</span>, <span class="string">'VALID'</span>) - mu_x * mu_y</span><br><span class="line"></span><br><span class="line">    SSIM_n = (<span class="number">2</span>*mu_x*mu_y + C1)*(<span class="number">2</span>*sigma_xy + C2)</span><br><span class="line">    SSIM_d = (mu_x**<span class="number">2</span> + mu_y**<span class="number">2</span> + C1)*(sigma_x + sigma_y + C2)</span><br><span class="line"></span><br><span class="line">    SSIM = SSIM_n/SSIM_d</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.clip_by_value((<span class="number">1</span> - SSIM)/<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="3-2-Disparity-Smoothness-Loss"><a href="#3-2-Disparity-Smoothness-Loss" class="headerlink" title="3.2 Disparity Smoothness Loss"></a>3.2 Disparity Smoothness Loss</h2><p><img src="/2018/06/19/monodepth/ds.png" alt="ds"><br>数字图像中，更多的使用差分来近似导数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_x</span><span class="params">(self, img)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> img[:,:,:<span class="number">-1</span>,:] - img[:,:,<span class="number">1</span>:,:];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_y</span><span class="params">(self, img)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> img[:,:<span class="number">-1</span>,:,:] - img[:,<span class="number">1</span>:,:,:];</span><br></pre></td></tr></table></figure></p>
<h2 id="3-3-Left-Right-Disparity-Consistency-Loss"><a href="#3-3-Left-Right-Disparity-Consistency-Loss" class="headerlink" title="3.3 Left-Right Disparity Consistency Loss"></a>3.3 Left-Right Disparity Consistency Loss</h2><p><img src="/2018/06/19/monodepth/lr.png" alt="lr"><br>这个表达式我自己认为是比较难理解，只要把它理解成是把右视图平移了之后要和左视图相等就可以了。作者的源码中是有一个函数<strong>generate_image_left(img, disp)</strong>，函数的作用是输入右图和左视差输出左图。这个损失函数的实现是利用这个函数输入右视差和左视差输出一个新的左视差，让这个新的左视差要等于预测出来的左视差。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.right_to_left_disp = [self.generate_image_left(self.disp_right_est[i], self.disp_left_est[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br><span class="line">self.lr_left_loss  = [tf.reduce_mean(tf.abs(self.right_to_left_disp[i] - self.disp_left_est[i]))   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br></pre></td></tr></table></figure></p>
<h1 id="4-阅读源码"><a href="#4-阅读源码" class="headerlink" title="4. 阅读源码"></a>4. 阅读源码</h1><h2 id="4-1-from-future-import-absolute-import的作用"><a href="#4-1-from-future-import-absolute-import的作用" class="headerlink" title="4.1 from future import absolute_import的作用"></a>4.1 from <strong>future</strong> import absolute_import的作用</h2><p>关于这句<strong>from <strong>future</strong> import absolute_import</strong>的作用:<br>直观地看就是说”加入<strong>绝对引入</strong>这个新特性”。说到<strong>绝对引入</strong>，当然就会想到<strong>相对引入</strong>。那么什么是<strong>相对引入</strong>呢?比如说，你的包结构是这样的:<br><strong>pkg/<br>pkg/init.py<br>pkg/main.py<br>pkg/string.py</strong><br>如果你在<strong>main.py</strong>中写<strong>import string</strong>,那么在<strong>Python 2.4</strong>或之前，<strong>Python</strong>会先查找当前目录下有没有<strong>string.py</strong>, 若找到了，则引入该模块，然后你在<strong>main.py</strong>中可以直接用<strong>string</strong>了。如果你是真的想用同目录下的<strong>string.py</strong>那就好，但是如果你是想用系统自带的标准<strong>string.py</strong>呢？这时候你就需要<strong>from <strong>future</strong> import absolute_import</strong>了。这样，你就可以用<strong>import string</strong>来引入系统的标准<strong>string.py</strong>, 而用<strong>from pkg import string</strong>来引入当前目录下的<strong>string.py</strong>了。</p>
<h2 id="4-2-os-environ-‘TF-CPP-MIN-LOG-LEVEL’-’1’"><a href="#4-2-os-environ-‘TF-CPP-MIN-LOG-LEVEL’-’1’" class="headerlink" title="4.2 os.environ[‘TF_CPP_MIN_LOG_LEVEL’]=’1’"></a>4.2 os.environ[‘TF_CPP_MIN_LOG_LEVEL’]=’1’</h2><p><strong>TF_CPP_MIN_LOG_LEVEL</strong>默认值为 0 (显示所有logs)，设置为 1 隐藏<strong>INFO logs</strong>, 2 额外隐藏<strong>WARNING logs</strong>, 设置为3接着额外隐藏<strong>ERROR logs</strong>。</p>
<h2 id="4-3-tf-app-run"><a href="#4-3-tf-app-run" class="headerlink" title="4.3 tf.app.run()"></a>4.3 tf.app.run()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
<p><strong>处理flag解析，然后执行main函数</strong>，那么flag解析是什么意思呢？诸如这样的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.app.flags.DEFINE_boolean(<span class="string">"self_test"</span>, <span class="keyword">False</span>, <span class="string">"True if running a self test."</span>)</span><br><span class="line">tf.app.flags.DEFINE_boolean(<span class="string">'use_fp16'</span>, <span class="keyword">False</span>,</span><br><span class="line">                            <span class="string">"Use half floats instead of full floats if True."</span>)</span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br></pre></td></tr></table></figure></p>
<h2 id="4-4-Tensorflow-about-Session-and-Graph"><a href="#4-4-Tensorflow-about-Session-and-Graph" class="headerlink" title="4.4 Tensorflow about Session and Graph"></a>4.4 Tensorflow about Session and Graph</h2><h3 id="4-4-1-Session"><a href="#4-4-1-Session" class="headerlink" title="4.4.1 Session"></a>4.4.1 Session</h3><p><strong>Session</strong>提供了<strong>Operation</strong>执行和<strong>Tensor</strong>求值的环境。如下面所示，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the graph in a session.</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the tensor 'c'.</span></span><br><span class="line"><span class="keyword">print</span> sess.run(c)</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># result: [3., 8.]</span></span><br></pre></td></tr></table></figure></p>
<p>一个Session可能会拥有一些资源，例如Variable或者Queue。当我们不再需要该session的时候，需要将这些资源进行释放。有两种方式，</p>
<ol>
<li>调用session.close()方法；</li>
<li>使用with tf.Session()创建上下文（Context）来执行，当上下文退出时自动释放。</li>
</ol>
<p>上面的例子可以写成,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">print</span> sess.run(c)</span><br></pre></td></tr></table></figure></p>
<p>Session类的构造函数如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Session.__init__(target=<span class="string">''</span>, graph=<span class="keyword">None</span>, config=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p>
<p>如果在创建Session时没有指定Graph，则该Session会加载默认Graph。如果在一个进程中创建了多个Graph，则需要创建不同的Session来加载每个Graph，而每个Graph则可以加载在多个Session中进行计算。<br>执行Operation或者求值Tensor有两种方式：</p>
<ol>
<li><p>调用Session.run()方法： 该方法的定义如下所示，参数fetches便是一个或者多个Operation或者Tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Session.run(fetches, feed_dict=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用Operation.run()或则Tensor.eval()方法： 这两个方法都接收参数session，用于指定在哪个session中计算。但该参数是可选的，默认为None，此时表示在进程默认session中计算。</p>
</li>
</ol>
<p>那如何设置一个Session为默认的Session呢？有两种方式：</p>
<ol>
<li><p>在with语句中定义的Session，在该上下文中便成为默认session；上面的例子可以修改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session():</span><br><span class="line">   <span class="keyword">print</span> c.eval()</span><br></pre></td></tr></table></figure>
</li>
<li><p>在with语句中调用Session.as_default()方法。 上面的例子可以修改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">c = a * b</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    <span class="keyword">print</span> c.eval()</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="5-4-2-Graph"><a href="#5-4-2-Graph" class="headerlink" title="5.4.2 Graph"></a>5.4.2 Graph</h3><p>Tensorflow中使用tf.Graph类表示可计算的图。图是由操作Operation和张量Tensor来构成，其中Operation表示图的节点（即计算单元），而Tensor则表示图的边（即Operation之间流动的数据单元）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Graph.__init__()</span><br></pre></td></tr></table></figure></p>
<p>上面创建一个空的Graph。</p>
<p>在Tensorflow中，始终存在一个默认的Graph。如果要将Operation添加到默认Graph中，只需要调用定义Operation的函数（例如tf.add()）。如果我们需要定义多个Graph，则需要在with语句中调用Graph.as_default()方法将某个graph设置成默认Graph，于是with语句块中调用的Operation或Tensor将会添加到该Graph中。<br>例如，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    c1 = tf.constant([<span class="number">1.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g2:</span><br><span class="line">    c2 = tf.constant([<span class="number">2.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess1:</span><br><span class="line">    <span class="keyword">print</span> sess1.run(c1)</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g2) <span class="keyword">as</span> sess2:</span><br><span class="line">    <span class="keyword">print</span> sess2.run(c2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result:</span></span><br><span class="line"><span class="comment"># [ 1.0 ]</span></span><br><span class="line"><span class="comment"># [ 2.0 ]</span></span><br></pre></td></tr></table></figure></p>
<p>如果将上面例子的sess1.run(c1)和sess2.run(c2)中的c1和c2交换一下位置，运行会报错。因为sess1加载的g1中没有c2这个Tensor，同样地，sess2加载的g2中也没有c1这个Tensor。</p>
<h2 id="5-5-tf-train-AdamOptimizer"><a href="#5-5-tf-train-AdamOptimizer" class="headerlink" title="5.5 tf.train.AdamOptimizer"></a>5.5 tf.train.AdamOptimizer</h2><p>目前其他我有看到的Optimizer还有</p>
<ol>
<li>GradientDescentOptimizer</li>
<li>MomentumOptimizer</li>
</ol>
<h2 id="5-6-tf-py-func"><a href="#5-6-tf-py-func" class="headerlink" title="5.6 tf.py_func"></a>5.6 tf.py_func</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.py_func(</span><br><span class="line">    func,</span><br><span class="line">    inp,</span><br><span class="line">    Tout,</span><br><span class="line">    stateful=<span class="keyword">True</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>给一个python自带或者numpy的函数func，把它变成tf的op。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_length_tf</span><span class="params">(t)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.py_func(len, [t], [tf.int64])</span><br></pre></td></tr></table></figure></p>
<p>像上面这样就可以输入一个tf对象，输出他的长度。</p>
<h2 id="5-6-tf-string-split"><a href="#5-6-tf-string-split" class="headerlink" title="5.6 tf.string_split"></a>5.6 tf.string_split</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.string_split(</span><br><span class="line">    source,</span><br><span class="line">    delimiter=<span class="string">' '</span>,</span><br><span class="line">    skip_empty=<span class="keyword">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>举例来说，N=2，source[0] = ‘hello world’，source[1] = ‘a b c’，<br>那么<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source = [<span class="string">'hello world'</span>, <span class="string">'a b c'</span>]</span><br><span class="line">st = tf.string_split(source)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(st.values))</span><br></pre></td></tr></table></figure></p>
<p>得到的结果是[b’hello’ b’word’ b’a’ b’b’ b’c’]。</p>
<h2 id="5-7-tf-string-join"><a href="#5-7-tf-string-join" class="headerlink" title="5.7 tf.string_join"></a>5.7 tf.string_join</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a=<span class="string">'aaaa'</span></span><br><span class="line">b=<span class="string">'b/'</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.string_join(b, a)))</span><br></pre></td></tr></table></figure>
<p>2018-07-15 13:36:44.843943: I T:\src\github\tensorflow\tensorflow\core\platform\<br>cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow bi<br>nary was not compiled to use: AVX2<br>Traceback (most recent call last):<br>  File “<stdin>“, line 2, in <module><br>  File “D:\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_string_ops.py”,<br> line 502, in string_join<br>    “‘string_join’ Op, not %r.” % inputs)<br>TypeError: Expected list for ‘inputs’ argument to ‘string_join’ Op, not ‘b/‘.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.string_join([b, a])))</span><br><span class="line"><span class="string">b'b/aaaa'</span></span><br></pre></td></tr></table></figure></module></stdin></p>
<p><strong>Expected list for ‘inputs’ argument to ‘string_join’ Op, not ‘b/‘.</strong>所以输入应该是以一个list的形式</p>
<h2 id="5-8-tf-random-uniform-0-1"><a href="#5-8-tf-random-uniform-0-1" class="headerlink" title="5.8 tf.random_uniform([], 0, 1)"></a>5.8 tf.random_uniform([], 0, 1)</h2><p>第一个参数是shape：<strong>A 1-D integer Tensor or Python array. The shape of the output tensor.</strong></p>
<h2 id="5-9-lambda"><a href="#5-9-lambda" class="headerlink" title="5.9 lambda"></a>5.9 lambda</h2><p>Python中，lambda函数也匿名函数。</p>
<p>lambda语法格式：<br>lambda 变量 : 要执行的语句</p>
<p>当使用tf.cond时，true_fn和false_fn都可以是lambda的形式，或者传入两个def的函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = tf.cond(x &lt; y, <span class="keyword">lambda</span>: tf.add(x, z), <span class="keyword">lambda</span>: tf.square(y))</span><br></pre></td></tr></table></figure></p>
<h2 id="5-10-tf-stack"><a href="#5-10-tf-stack" class="headerlink" title="5.10 tf.stack()"></a>5.10 tf.stack()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.constant([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">c = tf.stack([a,b], axis=<span class="number">0</span>)</span><br><span class="line">d = tf.unstack(c, axis=<span class="number">0</span>)</span><br><span class="line">e = tf.unstack(c, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(c))</span><br><span class="line">    print(sess.run(d))</span><br><span class="line">    print(sess.run(e))</span><br><span class="line">...</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">[array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])]</span><br><span class="line">[array([<span class="number">1</span>, <span class="number">4</span>]), array([<span class="number">2</span>, <span class="number">5</span>]), array([<span class="number">3</span>, <span class="number">6</span>])]</span><br></pre></td></tr></table></figure>
<h2 id="5-11-AttributeError-enter-错误解决"><a href="#5-11-AttributeError-enter-错误解决" class="headerlink" title="5.11 AttributeError enter__错误解决"></a>5.11 AttributeError enter__错误解决</h2><p>tf.Session()错写成tf.Session了。</p>
<h2 id="5-12-Python2和3中的除法运算"><a href="#5-12-Python2和3中的除法运算" class="headerlink" title="5.12 Python2和3中的除法运算"></a>5.12 Python2和3中的除法运算</h2><p>Python 2.x:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1</span> / <span class="number">2</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1.0</span> / <span class="number">2.0</span></span><br><span class="line"><span class="number">0.5</span></span><br></pre></td></tr></table></figure></p>
<p>Python 3.x:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1</span>/<span class="number">2</span></span><br><span class="line"><span class="number">0.5</span></span><br></pre></td></tr></table></figure></p>
<p>而对于//除法，这种除法叫做floor除法，会对除法的结果自动进行一个floor操作，在python 2.x和python 3.x中是一致的。<br>Python 2.x and Python 3.x:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">-1</span> // <span class="number">2</span></span><br><span class="line"><span class="number">-1</span></span><br></pre></td></tr></table></figure></p>
<p>注意的是并不是舍弃小数部分，而是执行floor操作，如果要截取小数部分，那么需要使用math模块的trunc函数。</p>
<h2 id="5-13-tf-variable-scope和tf-name-scope的用法"><a href="#5-13-tf-variable-scope和tf-name-scope的用法" class="headerlink" title="5.13 tf.variable_scope和tf.name_scope的用法"></a>5.13 tf.variable_scope和tf.name_scope的用法</h2><p>tf.variable_scope可以让变量有相同的命名，包括tf.get_variable得到的变量，还有tf.Variable的变量<br>tf.name_scope可以让变量有相同的命名，只是限于tf.Variable的变量<br><strong>reuse: True, None, or tf.AUTO_REUSE; if True, we go into reuse mode for this scope as well as all sub-scopes; if tf.AUTO_REUSE, we create variables if they do not exist, and return them otherwise; if None, we inherit the parent scope’s reuse flag. When eager execution is enabled, this argument is always forced to be tf.AUTO_REUSE.</strong><br>例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf  </span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'V1'</span>):</span><br><span class="line">  a1 = tf.get_variable(name=<span class="string">'a1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">  a2 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>), name=<span class="string">'a2'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'V2'</span>):</span><br><span class="line">  a3 = tf.get_variable(name=<span class="string">'a1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">  a4 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>), name=<span class="string">'a2'</span>)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.initialize_all_variables())</span><br><span class="line">  <span class="keyword">print</span> a1.name</span><br><span class="line">  <span class="keyword">print</span> a2.name</span><br><span class="line">  <span class="keyword">print</span> a3.name</span><br><span class="line">  <span class="keyword">print</span> a4.name</span><br></pre></td></tr></table></figure></p>
<p>输出：<br>V1/a1:0<br>V1/a2:0<br>V2/a1:0<br>V2/a2:0</p>
<p>例子2：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'V1'</span>):</span><br><span class="line">  a1 = tf.get_variable(name=<span class="string">'a1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">  a2 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>), name=<span class="string">'a2'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'V2'</span>):</span><br><span class="line">  a3 = tf.get_variable(name=<span class="string">'a1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">  a4 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>), name=<span class="string">'a2'</span>)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.initialize_all_variables())</span><br><span class="line">  <span class="keyword">print</span> a1.name</span><br><span class="line">  <span class="keyword">print</span> a2.name</span><br><span class="line">  <span class="keyword">print</span> a3.name</span><br><span class="line">  <span class="keyword">print</span> a4.name</span><br></pre></td></tr></table></figure></p>
<p>报错：ValueError: Variable a1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:</p>
<h2 id="5-14-tf-nn-elu"><a href="#5-14-tf-nn-elu" class="headerlink" title="5.14 tf.nn.elu"></a>5.14 tf.nn.elu</h2><p><img src="/2018/06/19/monodepth/elu.png" alt=""></p>
<h2 id="5-15-Python中append和extend函数区别"><a href="#5-15-Python中append和extend函数区别" class="headerlink" title="5.15 Python中append和extend函数区别"></a>5.15 Python中append和extend函数区别</h2><p>append函数直接将object整体当作一个元素追加到列表中，而extend函数则是将可迭代对象中的元素逐个追加到列表中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">lt1=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>]</span><br><span class="line">lt2=[<span class="string">'D'</span>,<span class="string">'E'</span>,<span class="string">'F'</span>]</span><br><span class="line">lt1.append(lt2)<span class="comment">#将lt2整体当作一个元素追加到到lt1中</span></span><br><span class="line">print(lt1)</span><br><span class="line">lt3=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>]</span><br><span class="line">lt2=[<span class="string">'D'</span>,<span class="string">'E'</span>,<span class="string">'F'</span>]</span><br><span class="line">lt3.extend(lt2)<span class="comment">#将lt2中每个元素逐个追加到t3中</span></span><br><span class="line">print(lt3)</span><br><span class="line">   ...:</span><br><span class="line">[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, [<span class="string">'D'</span>, <span class="string">'E'</span>, <span class="string">'F'</span>]]</span><br><span class="line">[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>, <span class="string">'F'</span>]</span><br></pre></td></tr></table></figure></p>
<h2 id="5-16-tf-concat"><a href="#5-16-tf-concat" class="headerlink" title="5.16 tf.concat"></a>5.16 tf.concat</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(</span><br><span class="line">    values,</span><br><span class="line">    axis,</span><br><span class="line">    name=<span class="string">'concat'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Concatenates tensors along one dimension.</strong></p>
<h2 id="5-17-tf-expand-dims"><a href="#5-17-tf-expand-dims" class="headerlink" title="5.17 tf.expand_dims"></a>5.17 tf.expand_dims</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims(</span><br><span class="line">    input,</span><br><span class="line">    axis=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    dim=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape [height, width, channels], you can make it a batch of 1 image with expand_dims(image, 0), which will make the shape [1, height, width, channels].</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">-1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 't2' is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="5-18-tf-cast"><a href="#5-18-tf-cast" class="headerlink" title="5.18 tf.cast"></a>5.18 tf.cast</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.cast(</span><br><span class="line">    x,</span><br><span class="line">    dtype,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Casts a tensor to a new type.</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1.8</span>, <span class="number">2.2</span>], dtype=tf.float32)</span><br><span class="line">tf.cast(x, tf.int32)  <span class="comment"># [1, 2], dtype=tf.int32</span></span><br></pre></td></tr></table></figure></p>
<h2 id="5-19-tf-meshgrid"><a href="#5-19-tf-meshgrid" class="headerlink" title="5.19 tf.meshgrid"></a>5.19 tf.meshgrid</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.meshgrid(</span><br><span class="line">    *args,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•*args: Tensors with rank 1.</span><br><span class="line">•**kwargs: - indexing: Either &apos;xy&apos; or &apos;ij&apos; (optional, default: &apos;xy&apos;).</span><br></pre></td></tr></table></figure>
<p>meshgrid支持笛卡尔坐标系(直角坐标系, ‘xy’)或是矩阵(‘ij’)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">y = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">X, Y = tf.meshgrid(x, y)</span><br><span class="line"><span class="comment"># X = [[1, 2, 3, 4],</span></span><br><span class="line"><span class="comment">#      [1, 2, 3, 4],</span></span><br><span class="line"><span class="comment">#      [1, 2, 3, 4]]</span></span><br><span class="line"><span class="comment"># Y = [[4, 4, 4, 4],</span></span><br><span class="line"><span class="comment">#      [5, 5, 5, 5],</span></span><br><span class="line"><span class="comment">#      [6, 6, 6, 6]]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="5-20-tf-linspace"><a href="#5-20-tf-linspace" class="headerlink" title="5.20 tf.linspace"></a>5.20 tf.linspace</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.linspace(</span><br><span class="line">    start,</span><br><span class="line">    stop,</span><br><span class="line">    num,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>•start: A Tensor. Must be one of the following types: bfloat16, float32, float64. First entry in the range.<br>•stop: A Tensor. Must have the same type as start. Last entry in the range.<br>•num: A Tensor. Must be one of the following types: int32, int64. Number of values to generate.</strong></p>
<h2 id="5-21-tf-reshape"><a href="#5-21-tf-reshape" class="headerlink" title="5.21 tf.reshape"></a>5.21 tf.reshape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.reshape(</span><br><span class="line">    tensor,</span><br><span class="line">    shape,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor 't' is [[[1, 1], [2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3], [4, 4]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [2, 2, 2]</span></span><br><span class="line">reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],</span><br><span class="line">                        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1, 1],</span></span><br><span class="line"><span class="comment">#                 [2, 2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3, 3],</span></span><br><span class="line"><span class="comment">#                 [4, 4, 4]],</span></span><br><span class="line"><span class="comment">#                [[5, 5, 5],</span></span><br><span class="line"><span class="comment">#                 [6, 6, 6]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [3, 2, 3]</span></span><br><span class="line"><span class="comment"># pass '[-1]' to flatten 't'</span></span><br><span class="line">reshape(t, [-1])    ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]</span><br><span class="line">reshape(t, [1, -1]) ==&gt; [[1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 6 6 6]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># -1 can also be used to infer the shape</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -1 is inferred to be 9:</span></span><br><span class="line">reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br></pre></td></tr></table></figure>
<h2 id="5-22-tf-tile"><a href="#5-22-tf-tile" class="headerlink" title="5.22 tf.tile"></a>5.22 tf.tile</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.tile(</span><br><span class="line">    input,</span><br><span class="line">    multiples,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>The output tensor’s i’th dimension has input.dims(i) × multiples[i] elements, and the values of input are replicated multiples[i] times along the ‘i’th dimension.</strong></p>
<p>•input: A Tensor. 1-D or higher.<br>•multiples: <strong>A Tensor</strong>. Must be one of the following types: int32, int64. 1-D. Length must be the same as the number of dimensions in input</p>
<h2 id="5-23-tf-stack"><a href="#5-23-tf-stack" class="headerlink" title="5.23 tf.stack"></a>5.23 tf.stack</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.stack(</span><br><span class="line">    values,</span><br><span class="line">    axis=<span class="number">0</span>,</span><br><span class="line">    name=<span class="string">'stack'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">y = tf.constant([<span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line">z = tf.constant([<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">tf.stack([x, y, z])  <span class="comment"># [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)</span></span><br><span class="line">tf.stack([x, y, z], axis=<span class="number">1</span>)  <span class="comment"># [[1, 2, 3], [4, 5, 6]]</span></span><br></pre></td></tr></table></figure>
<h2 id="5-24-tf-range"><a href="#5-24-tf-range" class="headerlink" title="5.24 tf.range"></a>5.24 tf.range</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.range(limit, delta=<span class="number">1</span>, dtype=<span class="keyword">None</span>, name=<span class="string">'range'</span>)</span><br><span class="line">tf.range(start, limit, delta=<span class="number">1</span>, dtype=<span class="keyword">None</span>, name=<span class="string">'range'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="5-25-tf-gather"><a href="#5-25-tf-gather" class="headerlink" title="5.25 tf.gather"></a>5.25 tf.gather</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.gather(</span><br><span class="line">    params,</span><br><span class="line">    indices,</span><br><span class="line">    validate_indices=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    axis=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/06/19/monodepth/gather.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>temp = tf.range(<span class="number">0</span>,<span class="number">10</span>)*<span class="number">10</span> + tf.constant(<span class="number">1</span>,shape=[<span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>temp = tf.tile(tf.expand_dims(temp, <span class="number">1</span>), [<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>temp2 = tf.gather(temp,[<span class="number">1</span>,<span class="number">5</span>,<span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="meta">... </span>    print(sess.run(temp))</span><br><span class="line"><span class="meta">... </span>    print(sess.run(temp2))</span><br><span class="line">...</span><br><span class="line">[[ <span class="number">1</span>  <span class="number">1</span>]</span><br><span class="line"> [<span class="number">11</span> <span class="number">11</span>]</span><br><span class="line"> [<span class="number">21</span> <span class="number">21</span>]</span><br><span class="line"> [<span class="number">31</span> <span class="number">31</span>]</span><br><span class="line"> [<span class="number">41</span> <span class="number">41</span>]</span><br><span class="line"> [<span class="number">51</span> <span class="number">51</span>]</span><br><span class="line"> [<span class="number">61</span> <span class="number">61</span>]</span><br><span class="line"> [<span class="number">71</span> <span class="number">71</span>]</span><br><span class="line"> [<span class="number">81</span> <span class="number">81</span>]</span><br><span class="line"> [<span class="number">91</span> <span class="number">91</span>]]</span><br><span class="line">[[<span class="number">11</span> <span class="number">11</span>]</span><br><span class="line"> [<span class="number">51</span> <span class="number">51</span>]</span><br><span class="line"> [<span class="number">91</span> <span class="number">91</span>]]</span><br></pre></td></tr></table></figure>
<h2 id="5-26-tf-reduce-mean"><a href="#5-26-tf-reduce-mean" class="headerlink" title="5.26 tf.reduce_mean"></a>5.26 tf.reduce_mean</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_mean(</span><br><span class="line">    input_tensor,</span><br><span class="line">    axis=<span class="keyword">None</span>,</span><br><span class="line">    keepdims=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    reduction_indices=<span class="keyword">None</span>,</span><br><span class="line">    keep_dims=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Computes the mean of elements across dimensions of a tensor. (deprecated arguments)</strong></p>
<h2 id="5-27-tf-train-shuffle-batch"><a href="#5-27-tf-train-shuffle-batch" class="headerlink" title="5.27 tf.train.shuffle_batch"></a>5.27 tf.train.shuffle_batch</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data = np.arange(<span class="number">1</span>, <span class="number">100</span> + <span class="number">1</span>)</span><br><span class="line">data_input = tf.constant(data)</span><br><span class="line"></span><br><span class="line">batch_shuffle = tf.train.shuffle_batch([data_input], enqueue_many=<span class="keyword">True</span>, batch_size=<span class="number">10</span>, capacity=<span class="number">100</span>, min_after_dequeue=<span class="number">10</span>, allow_smaller_final_batch=<span class="keyword">True</span>)</span><br><span class="line">batch_no_shuffle = tf.train.batch([data_input], enqueue_many=<span class="keyword">True</span>, batch_size=<span class="number">10</span>, capacity=<span class="number">100</span>, allow_smaller_final_batch=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    coord = tf.train.Coordinator()</span><br><span class="line">    threads = tf.train.start_queue_runners(coord=coord)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        print(i, sess.run([batch_shuffle, batch_no_shuffle]))</span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br></pre></td></tr></table></figure>
<p><strong>Which yields:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">0 [array([23, 48, 15, 46, 78, 89, 18, 37, 88,  4]), array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]</span><br><span class="line">1 [array([80, 10,  5, 76, 50, 53,  1, 72, 67, 14]), array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20])]</span><br><span class="line">2 [array([11, 85, 56, 21, 86, 12,  9,  7, 24,  1]), array([21, 22, 23, 24, 25, 26, 27, 28, 29, 30])]</span><br><span class="line">3 [array([ 8, 79, 90, 81, 71,  2, 20, 63, 73, 26]), array([31, 32, 33, 34, 35, 36, 37, 38, 39, 40])]</span><br><span class="line">4 [array([84, 82, 33,  6, 39,  6, 25, 19, 19, 34]), array([41, 42, 43, 44, 45, 46, 47, 48, 49, 50])]</span><br><span class="line">5 [array([27, 41, 21, 37, 60, 16, 12, 16, 24, 57]), array([51, 52, 53, 54, 55, 56, 57, 58, 59, 60])]</span><br><span class="line">6 [array([69, 40, 52, 55, 29, 15, 45,  4,  7, 42]), array([61, 62, 63, 64, 65, 66, 67, 68, 69, 70])]</span><br><span class="line">7 [array([61, 30, 53, 95, 22, 33, 10, 34, 41, 13]), array([71, 72, 73, 74, 75, 76, 77, 78, 79, 80])]</span><br><span class="line">8 [array([45, 52, 57, 35, 70, 51,  8, 94, 68, 47]), array([81, 82, 83, 84, 85, 86, 87, 88, 89, 90])]</span><br><span class="line">9 [array([35, 28, 83, 65, 80, 84, 71, 72, 26, 77]), array([ 91,  92,  93,  94,  95,  96,  97,  98,  99, 100])]</span><br></pre></td></tr></table></figure></p>
<h2 id="5-28-tf-add-n"><a href="#5-28-tf-add-n" class="headerlink" title="5.28 tf.add_n"></a>5.28 tf.add_n</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.add_n(</span><br><span class="line">    inputs,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>inputs: A list of Tensor objects, each with same shape and type.</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf;</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line"> </span><br><span class="line">input1 = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">input2 = tf.Variable(tf.random_uniform([<span class="number">3</span>]))</span><br><span class="line">output = tf.add_n([input1, input2])</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.initialize_all_variables())</span><br><span class="line">  <span class="keyword">print</span> sess.run(input1 + input2)</span><br><span class="line">  <span class="keyword">print</span> sess.run(output)</span><br></pre></td></tr></table></figure></p>
<h2 id="5-28-squeeze"><a href="#5-28-squeeze" class="headerlink" title="5.28 squeeze"></a>5.28 squeeze</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.squeeze(</span><br><span class="line">    input,</span><br><span class="line">    axis=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    squeeze_dims=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed. </strong></p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/深度估计/">深度估计</a>
  </div>




<div class="article-share" id="share">

  <div data-url="https://www.lijn.tech/2018/06/19/monodepth/" data-title="Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral | LI jianan&#39;s Blog" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/06/20/bilinear/" title="Bilinear Interpolation">
  <strong>Previous:</strong><br/>
  <span>
  Bilinear Interpolation</span>
</a>
</div>


<div class="next">
<a href="/2018/06/14/ccbdai/"  title="云计算、大数据和人工智能">
 <strong>Next:</strong><br/> 
 <span>云计算、大数据和人工智能
</span>
</a>
</div>

</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span> If you are interested in Computer Vision, you can contact </span>
			<span> <a href="https://github.com/JnanLi">me.</a> </span>
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>




<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  </body>
</html>
