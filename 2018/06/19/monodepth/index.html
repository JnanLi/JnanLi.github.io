
 <!DOCTYPE HTML>
<html >
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral | LI jianan&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="LI jianan">
    
    <meta name="description" content="本文采用无监督学习的方法来估计深度，基本思路是匹配好左右视图的像素，得到disparity map。根据得到的视差disparity，由d = bf/disparity，算出depth map。本文能实现在35ms内恢复一张图512×256的图只需要25ms（GPU）。               ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.jpg">
    

  
  

    <link rel="stylesheet" href="/css/style.css">
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="LI jianan&#39;s Blog">LI jianan&#39;s Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/06/19/monodepth/" title="Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral" itemprop="url">Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral</a>
  </h1>
  <p class="article-time">
    <time datetime="2018-06-19T07:20:52.000Z" itemprop="datePublished">2018-06-19</time>
  </p>
</header>
	<div class="article-content">
		
		
		<p>本文采用无监督学习的方法来估计深度，基本思路是匹配好左右视图的像素，得到disparity map。根据得到的视差disparity，由d = bf/disparity，算出depth map。本文能实现在35ms内恢复一张图512×256的图只需要25ms（GPU）。                                                                                                                                                                                                                                                                                   </p>
<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h1><p>我们提出了一种新的训练目标，使得我们的卷积神经网络能够学习执行单个图像深度估计，尽管缺乏ground truth。Exploiting epipolar geometry constraints，我们通过训练我们的网络以产生图像重构损失来生成视差图像。我们表明，单独解决图像重建会导致质量较差的深度图像。为了克服这个问题，我们提出了一种新的训练损失，它强化了相对于左侧和右侧图像产生的差异之间的一致性，与现有方法相比改进了性能和鲁棒性。</p>
<h1 id="2-Key-Sentences"><a href="#2-Key-Sentences" class="headerlink" title="2. Key Sentences"></a>2. Key Sentences</h1><p><strong>In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage.</strong></p>
<p><strong>We show that solving for image reconstruction alone re-sults in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency be-tween the disparities produced relative to both the left and right images, leading to improved performance and robustness com-pared to existing approaches.</strong> </p>
<p><strong>Our method is fast and only takes on the order of 35 milliseconds to predict a dense depth map for a 512×256 image on a modern GPU.</strong> </p>
<p><strong>Here we focus on works related to monocular depth estimation, where there is only a single input image, and no assumptions about the scene geometry or types of objects present are made.</strong></p>
<p><strong>The key insight of our method is that we can simultaneously infer both disparities (left-to-right and right-to-left), using only the left input image, and obtain better depths by enforcing them to be consistent with each other.</strong></p>
<p><strong>We want the output disparity map to align with the input left image, meaning the network has to sample from the right image.</strong></p>
<h1 id="3-DispNet"><a href="#3-DispNet" class="headerlink" title="3. DispNet"></a>3. DispNet</h1><p><a href="https://ieeexplore.ieee.org/document/7780807/" target="_blank" rel="noopener">A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</a></p>
<h1 id="3-1-Main-contributions"><a href="#3-1-Main-contributions" class="headerlink" title="3.1 Main contributions:"></a>3.1 Main contributions:</h1><p><strong>Our datasets are the first large-scale datasets to enable training and evaluation of scene flow methods.</strong><br><strong>Present a convolutional network for real-time disparity estimation that provides state-of-the-art results.</strong></p>
<p>By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.</p>
<h1 id="3-2-scene-flow的定义"><a href="#3-2-scene-flow的定义" class="headerlink" title="3.2 scene flow的定义"></a>3.2 scene flow的定义</h1><p>Optimal flow是3D运动在2D平面上的投影。Scene flow被认为是可以从立体视频或者RGBD视频中计算出的潜在3D运动场。假定有t和t+1时间的立体图像对，即有(I<sup>t</sup><sub>L</sub>, I<sup>t</sup><sub>R</sub>, I<sup>t+1</sup><sub>L</sub>, I<sup>t+1</sup><sub>R</sub>)。Scene flow在已知相机内外部参数的情况下，为四张图像中每一个可见点提供其3D位置和3D运动向量。</p>
<p><img src="/2018/06/19/monodepth/Scene.gif" alt="Scene Flow"></p>
<p>红色箭头表示就是来估计scene flow的。</p>
<h1 id="3-3-论文提出了三个数据子集"><a href="#3-3-论文提出了三个数据子集" class="headerlink" title="3.3 论文提出了三个数据子集"></a>3.3 论文提出了三个数据子集</h1><p>The datasets are freely available online：<br><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/" target="_blank" rel="noopener">传送门</a></p>
<h1 id="3-4-DispNet"><a href="#3-4-DispNet" class="headerlink" title="3.4 DispNet"></a>3.4 DispNet</h1><p>对于视差估计，作者提出DispNet网络结构框架：<br><img src="/2018/06/19/monodepth/DispNet.JPG" alt="DispNet"></p>
<h1 id="4-Training-Loss"><a href="#4-Training-Loss" class="headerlink" title="4. Training Loss"></a>4. Training Loss</h1><p>We define a loss C<sub>s</sub> at each output scale s, forming the total loss as the sum C = C<sub>1</sub> + … + C<sub>4</sub>. Our loss module (below) computes C<sub>s</sub> as combination of three main terms.</p>
<p><img src="/2018/06/19/monodepth/lossmodule.gif" alt="lossmodule"><br>Our loss module outputs left and right disparity maps, d<sup>l</sup> and d<sup>r</sup>. The loss combines smoothness, reconstruction, and left-right disparity consistency terms. This same module is repeated at each of the four different output scales. C: Convolution, UC: Up-convolution, S: Bilinear sampling, US: Up-Sampling, SC: Skip connection.</p>
<p><img src="/2018/06/19/monodepth/lossall.png" alt="lossall"></p>
<p>分别是：<br><strong>左右视图的灰度匹配部分</strong><br><strong>视差平滑部分（让disparity的分布更加平滑）</strong><br><strong>左右视图的一致性部分(促使左视图中的disparity分布和右视图的disparity图严格相同)</strong></p>
<h2 id="4-1-左右视图的灰度匹配部分"><a href="#4-1-左右视图的灰度匹配部分" class="headerlink" title="4.1 左右视图的灰度匹配部分"></a>4.1 左右视图的灰度匹配部分</h2><p><img src="/2018/06/19/monodepth/ap.png" alt="ap"></p>
<p>本文采用和Loss Functions for Neural Networks for Image Processing一文相同的形式，将SSIM和L1结合起来作为代价函数，其中α设置为0.85。<br>此式包含了L1和SSIM项两部分。</p>
<h2 id="4-2-视差平滑部分"><a href="#4-2-视差平滑部分" class="headerlink" title="4.2 视差平滑部分"></a>4.2 视差平滑部分</h2><p><img src="/2018/06/19/monodepth/ds.png" alt="ds"></p>
<p>数字图像中，更多的使用差分来近似导数，最简单的梯度近似表达式如下：</p>
<p><img src="/2018/06/19/monodepth/tidu.png" alt="tidu"></p>
<h2 id="4-3-左右视图的一致性部分"><a href="#4-3-左右视图的一致性部分" class="headerlink" title="4.3 左右视图的一致性部分"></a>4.3 左右视图的一致性部分</h2><p><img src="/2018/06/19/monodepth/lr.png" alt="lr"></p>
<p><a href="https://ieeexplore.ieee.org/document/8100182/" target="_blank" rel="noopener">Unsupervised Monocular Depth Estimation with Left-Right Consistency</a></p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/Depth-Estimation/">Depth Estimation</a>
  </div>




<div class="article-share" id="share">

  <div data-url="https://www.lijn.tech/2018/06/19/monodepth/" data-title="Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral | LI jianan&#39;s Blog" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/06/19/network-configuration/" title="虚拟机网络配置详解(NAT、桥接、Hostonly)">
  <strong>Previous:</strong><br/>
  <span>
  虚拟机网络配置详解(NAT、桥接、Hostonly)</span>
</a>
</div>


<div class="next">
<a href="/2018/06/14/ccbdai/"  title="云计算、大数据和人工智能">
 <strong>Next:</strong><br/> 
 <span>云计算、大数据和人工智能
</span>
</a>
</div>

</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span> If you are interested in Computer Vision, you can contact </span>
			<span> <a href="https://github.com/JnanLi">me.</a> </span>
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>




<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  </body>
</html>
