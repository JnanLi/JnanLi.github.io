
 <!DOCTYPE HTML>
<html >
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>深度相机 | LI jianan&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="LI jianan">
    
    <meta name="description" content="深度相机了解一下？                                                                                                                                             ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.jpg">
    

  
  

    <link rel="stylesheet" href="/css/style.css">
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="LI jianan&#39;s Blog">LI jianan&#39;s Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/28/depthcamera/" title="深度相机" itemprop="url">深度相机</a>
  </h1>
  <p class="article-time">
    <time datetime="2018-07-28T08:57:10.000Z" itemprop="datePublished">2018-07-28</time>
  </p>
</header>
	<div class="article-content">
		
		
		<p>深度相机了解一下？                                                                                                                                                                                                                                                                                                                                                                             </p>
<h1 id="深度相机原理——飞行时间TOF"><a href="#深度相机原理——飞行时间TOF" class="headerlink" title="深度相机原理——飞行时间TOF"></a>深度相机原理——飞行时间TOF</h1><p>深度相机按照深度测量原理不同，一般分为：飞行时间法、结构光法、双目立体视觉法。</p>
<p>飞行时间基本原理是通过连续发射光脉冲（一般为不可见光）到被观测物体上，然后接收从物体反射回去的光脉冲，通过探测光脉冲的飞行（往返）时间来计算被测物体离相机的距离。</p>
<p><img src="/2018/07/28/depthcamera/TOF.webp" alt=""></p>
<p>TOF法根据调制方法的不同，一般可以分为两种：脉冲调制（Pulsed Modulation）和连续波调制（Continuous Wave Modulation）。</p>
<p>目前的消费级TOF深度相机主要有：微软的Kinect 2、 MESA 的 SR4000 、Google Project Tango 中使用的PMD Tech 的TOF深度相机等。这些产品已经在体感识别、手势识别、环境建模等方面取得了较多的应用，最典型的就是微软的Kinect 2。</p>
<p>TOF深度相机对时间测量的精度要求较高，即使采用最高精度的电子元器件，也很难达到毫米级的精度。因此，在近距离测量领域，尤其是1m范围内，TOF深度相机的精度与其他深度相机相比还具有较大的差距，<strong>这限制它在近距离高精度领域的应用。</strong></p>
<p>但是，从前面的原理不难看出，<strong>TOF深度相机可以通过调节发射脉冲的频率改变相机测量距离</strong>；TOF深度相机与基于特征匹配原理的深度相机不同，其测量精度不会随着测量距离的增大而降低，<strong>其测量误差在整个测量范围内基本上是固定的</strong>；TOF深度相机抗干扰能力也较强。因此，<strong>在测量距离要求比较远的场合（如无人驾驶），TOF深度相机具有非常明显的优势。</strong></p>
<h1 id="深度相机原理——双目立体视觉"><a href="#深度相机原理——双目立体视觉" class="headerlink" title="深度相机原理——双目立体视觉"></a>深度相机原理——双目立体视觉</h1><p>基于双目立体视觉的深度相机类似人类的眼睛，和基于TOF、结构光原理的深度相机不同，它不对外主动投射光源，完全依靠拍摄的两张图片(RGB或灰度图)来计算深度，因此有时候被称为双目深度相机。比较知名的产品有STEROLABS 推出的 ZED 2K Stereo Camera和Point Grey 公司推出的 BumbleBee。</p>
<h2 id="为什么非得用双目相机才能获得深度？"><a href="#为什么非得用双目相机才能获得深度？" class="headerlink" title="为什么非得用双目相机才能获得深度？"></a>为什么非得用双目相机才能获得深度？</h2><p>说到这里，有些读者会问啦：为什么非得用双目相机才能得到深度？我闭上一只眼只用一只眼来观察，也能知道哪个物体离我近哪个离我远啊！是不是说明单目相机也可以获得深度？</p>
<p>在此解答一下：首先，确实人通过一只眼也可以获得一定的深度信息，不过这背后其实有一些容易忽略的因素在起作用：<strong>一是因为人本身对所处的世界是非常了解的（先验知识），因而对日常物品的大小是有一个基本预判的（从小到大多年的视觉训练），根据近大远小的常识确实可以推断出图像中什么离我们远什么离我们近；二是人在单眼观察物体的时候其实人眼是晃动的，相当于一个移动的单目相机</strong>，这类似于运动恢复结构（Structure from Motion, SfM）的原理，移动的单目相机通过比较多帧差异确实可以得到深度信息。</p>
<p>但是实际上，相机毕竟不是人眼，它只会傻傻的按照人的操作拍照，不会学习和思考。下图从物理原理上展示了为什么单目相机不能测量深度值而双目可以的原因。我们看到红色线条上三个不同远近的黑色的点在下方相机上投影在同一个位置，因此单目相机无法分辨成的像到底是远的那个点还是近的那个点，但是它们在上方相机的投影却位于三个不同位置，因此通过两个相机的观察可以确定到底是哪一个点。</p>
<p><img src="/2018/07/28/depthcamera/binocular.webp" alt=""></p>
<h2 id="双目相机简化流程"><a href="#双目相机简化流程" class="headerlink" title="双目相机简化流程"></a>双目相机简化流程</h2><ol>
<li><p>首先需要对双目相机进行标定，得到两个相机的内外参数、单应矩阵。</p>
</li>
<li><p>根据标定结果对原始图像校正，校正后的两张图像位于同一平面且互相平行。</p>
</li>
<li><p>对校正后的两张图像进行像素点匹配。</p>
</li>
<li><p>根据匹配结果计算每个像素的深度，从而获得深度图。</p>
</li>
</ol>
<h2 id="双目相机原理"><a href="#双目相机原理" class="headerlink" title="双目相机原理"></a>双目相机原理</h2><h3 id="理想双目相机成像原理"><a href="#理想双目相机成像原理" class="headerlink" title="理想双目相机成像原理"></a>理想双目相机成像原理</h3><p>首先我们从理想的情况开始分析:假设左右两个相机位于同一平面（光轴平行），且相机参数（如焦距f）一致。那么深度值的推导原理和公式如下。</p>
<p><img src="/2018/07/28/depthcamera/binocular2.webp" alt=""></p>
<p>根据上述推导，空间点P离相机的距离（深度）z=fb/d，可以发现如果要计算深度z，必须要知道：</p>
<ol>
<li><p>相机焦距f，左右相机基线b。这些参数可以通过先验信息或者相机标定得到。</p>
</li>
<li><p>视差d。需要知道左相机的每个像素点(xl, yl)和右相机中对应点(xr, yr)的对应关系。这是双目视觉的核心问题。</p>
</li>
</ol>
<h3 id="极线约束"><a href="#极线约束" class="headerlink" title="极线约束"></a>极线约束</h3><p>那么问题来了，对于左图中的一个像素点，如何确定该点在右图中的位置？是不是需要我们在整个图像中地毯式搜索一个个匹配？</p>
<p>答案是：不需要。因为有极线约束（名字听着很吓人）。<strong>极线约束对于求解图像对中像素点的对应关系非常重要。</strong></p>
<p>那什么是极线呢？如下图所示。</p>
<p><img src="/2018/07/28/depthcamera/jixian.webp" alt=""></p>
<p>C1和C2是两个相机，P是空间中的一个点，<em>P和两个相机中心点C1、C2形成了三维空间中的一个平面PC1C2，称为极平面。极平面与两幅图像相交于两条直线，这两条直线称为极线。</em>P在相机C1中的成像点是P1，在相机C2中的成像点是P2，但是P的位置事先未知。</p>
<p><em>我们的目标是：对于左图的P1，寻找它在右图中对应点P2，这样就能确定P点的空间位置，也就是我们想要的空间物体和相机的距离。</em></p>
<p><em>极线约束就是当同一个空间点在两幅图像上分别成像时，已知左图投影点P1，那么对应右图投影点P2一定在P1的极线上，</em>这可以极大的缩小匹配范围。</p>
<p>上述考虑的情况非常理想，相机C1、C2不在同一条直线上怎么办？</p>
<p><em>这种情况非常常见，因为有些场景下两个相机需要独立固定，很难保证光心C1、C2完全水平，即使是固定在同一个基板上也会因为装配的原因导致光心不完全水平。如下图所示，我们看到两个相机的极线不仅不平行，还不共面。</em>之前的理想模型用不了了，怎么办？</p>
<p><img src="/2018/07/28/depthcamera/jixian2.webp" alt=""></p>
<p>我们先来看看这种情况下拍摄的两张左右图片吧，左图中三个十字标志的点，在右图中对应的极线是右图中的三条白色直线，也就是对应的搜索区域。</p>
<p><img src="/2018/07/28/depthcamera/rectify1.webp" alt=""></p>
<h3 id="图像矫正技术"><a href="#图像矫正技术" class="headerlink" title="图像矫正技术"></a>图像矫正技术</h3><p><em>图像矫正是通过分别对两张图片用单应(homography)矩阵变换(可以通过标定获得)得到的，目的就是把两个不同方向的图像平面(灰色平面)重新投影到同一个平面且光轴互相平行(黄色平面)，</em>这样就可以用前面的理想情况下的模型了。</p>
<p><img src="/2018/07/28/depthcamera/rectify.webp" alt=""></p>
<p>经过图像矫正后，左图中的像素点只需要沿着水平的极线方向搜索对应点就可以了（开心）。从下图中我们可以看到三个点对应的视差（红色双箭头线段）是不同的，越远的物体视差越小，越近的物体视差越大，这和我们的常识是一致的。</p>
<p><img src="/2018/07/28/depthcamera/rectify2.webp" alt=""></p>
<p>上面讲到的对于左图的一个点，沿着它在右图中水平极线方向寻找和它最匹配的像素点，说起来简单，实际操作起来却不容易。这是因为上述都是理想情况下的假设。实际进行像素点匹配的时候会发现几个问题：</p>
<ol>
<li><p>实际上要保证两个相机完全共面且参数一致是非常困难的，而且计算过程中也会产生误差累积，因此对于左图的一个点，其在右图的对应点不一定恰好在极线上。但是应该是在极线附近，所以搜索范围需要适当放宽。</p>
</li>
<li><p>单个像素点进行比较鲁棒性很差，很容易受到光照变化和视角不同的影响。</p>
</li>
</ol>
<h3 id="基于滑动窗口的图像匹配"><a href="#基于滑动窗口的图像匹配" class="headerlink" title="基于滑动窗口的图像匹配"></a>基于滑动窗口的图像匹配</h3><p>上述问题的解决方法：使用滑动窗口来进行匹配。如下图所示。对于左图中的？一个？像素点（左图中红色方框中心），在右图中从左到右用一个同尺寸滑动窗口内的像素和它计算相似程度，相似度的度量有很多种方法，比如 误差平方和法（Sum of Squared Differences，简称SSD），左右图中两个窗口越相似，SSD越小。下图中下方的SSD曲线显示了计算结果，SSD值最小的位置对应的像素点就是最佳的匹配结果。</p>
<p><img src="/2018/07/28/depthcamera/slide.webp" alt=""></p>
<p>具体操作中还有很多实际问题，比如滑动窗口尺寸。滑动窗口的大小选取还是很有讲究的。下图显示了不同尺寸的滑动窗口对深度图计算结果的影响。从图中我们也不难发现:</p>
<p>小尺寸的窗口：精度更高、细节更丰富；但是对噪声特别敏感<br>大尺寸的窗口：精度不高、细节不够；但是对噪声比较鲁棒</p>
<p><img src="/2018/07/28/depthcamera/slide2.webp" alt=""></p>
<p>虽然基于滑动窗口的匹配方法可以计算得到深度图，但是这种方法匹配效果并不好，而且由于要逐点进行滑动窗口匹配，计算效率也很低。</p>
<h3 id="基于能量优化的图像匹配"><a href="#基于能量优化的图像匹配" class="headerlink" title="基于能量优化的图像匹配"></a>基于能量优化的图像匹配</h3><p><em>目前比较主流的方法都是基于能量优化的方法来实现匹配的，能量优化通常会定义一个能量函数。比如对于两张图中像素点的匹配问题来说，我们定义的能量函数如下图公式1，我们的目的是：</em></p>
<ol>
<li>在左图中所有的像素点和右图中对应的像素点越近似越好，反映在图像里就是灰度值越接近越好，也就是下图公式2的描述。</li>
<li>在同一张图片里，两个相邻的像素点视差（深度值）也应该相近。也就是下图公式3的描述。</li>
</ol>
<p><img src="/2018/07/28/depthcamera/energy.webp" alt=""></p>
<p>通过对能量函数最小化，我们最后得到了一个最佳的匹配结果。</p>
<h3 id="双目立体视觉优缺点"><a href="#双目立体视觉优缺点" class="headerlink" title="双目立体视觉优缺点"></a>双目立体视觉优缺点</h3><p>优点：</p>
<ol>
<li>对相机硬件要求低，成本也低，因为不需要像TOF和结构光那样使用发射器和接收器，使用普通的消费级RGB相机即可。</li>
<li>室内外都适用，由于直接根据环境光采集图像，所以在室内、室外都能使用。相比之下，TOF和结构光只能在室内使用。</li>
</ol>
<p>缺点：</p>
<ol>
<li>对环境光照非常敏感，双目立体视觉依赖环境中的自然光线采集图像，而由于光照角度变化、光照强度变化等环境因素的影响，拍摄的两张图片亮度差别会比较大，这会对匹配算法提出很大的挑战。如下图是在不同光照条件下拍摄的图片：<br><img src="/2018/07/28/depthcamera/illumination.webp" alt=""></li>
<li>不适用于单调缺乏纹理的场景。</li>
<li>计算复杂度高。该方法是纯视觉的方法，需要逐像素计算匹配；又因为上述多种因素的影响，需要保证匹配结果比较鲁棒，所以算法中会增加大量的错误剔除策略，因此对算法要求较高，想要实现可靠商用难度大，计算量较大。</li>
<li>相机基线限制了测量范围。测量范围和基线（两个摄像头间距）关系很大：基线越大，测量范围越远；基线越小，测量范围越近。所以基线在一定程度上限制了该深度相机的测量范围。</li>
</ol>
<p><em>?基于能量优化的图像匹配是怎么实现，给你左右视图如何得到视差?</em></p>
<h1 id="深度相机原理——结构光"><a href="#深度相机原理——结构光" class="headerlink" title="深度相机原理——结构光"></a>深度相机原理——结构光</h1><p>双目相机对环境光照强度比较敏感，且比较依赖图像本身的特征，因此<em>在光照不足、缺乏纹理等情况下</em>很难提取到有效鲁棒的特征，从而导致匹配误差增大甚至匹配失败。</p>
<p>而基于结构光的深度相机就是为了解决上述双目匹配算法的复杂度和鲁棒性问题而提出的，结构光法不依赖于物体本身的颜色和纹理，<em>采用了主动投影已知图案的方法来实现快速鲁棒的匹配特征点，能够达到较高的精度，也大大扩展了适用范围。</em></p>
<p>下图左是普通双目立体视觉深度相机拍摄的图像和对应的深度图结果；下图右是结构光法的深度相机投射的图案及对应的深度图结果，明显可以观察到在同样的场景下结构光法得到的深度图更完整，细节更丰富，效果大大好于双目立体视觉法。</p>
<p><img src="/2018/07/28/depthcamera/jgg.webp" alt=""></p>
<h2 id="结构光优缺点总结"><a href="#结构光优缺点总结" class="headerlink" title="结构光优缺点总结"></a>结构光优缺点总结</h2><p>优点</p>
<ol>
<li>由于结构光主动投射编码光，因而非常适合在光照不足(甚至无光)、缺乏纹理的场景使用。</li>
<li>结构光投图案一般经过精心设计，所以在一定范围内可以达到较高的测量精度。</li>
<li>较高的分辨率</li>
</ol>
<p>缺点：</p>
<ol>
<li>室外环境基本不能使用。这是因为在室外容易受到强自然光影响，导致投射的编码光被淹没。增加投射光源的功率可以一定程度上缓解该问题，但是效果并不能让人满意。</li>
<li>测量距离较近。物体距离相机越远，物体上的投影图案越大，精度也越差（想象一下手电筒照射远处的情景），相对应的测量精度也越差。所以基于结构光的深度相机测量精度随着距离的增大而大幅降低。因而，往往在近距离场景中应用较多。</li>
<li>容易受到光滑平面反光的影响。</li>
</ol>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/深度估计/">深度估计</a>
  </div>




<div class="article-share" id="share">

  <div data-url="https://www.lijn.tech/2018/07/28/depthcamera/" data-title="深度相机 | LI jianan&#39;s Blog" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/07/30/interview/" title="面经">
  <strong>Previous:</strong><br/>
  <span>
  面经</span>
</a>
</div>


<div class="next">
<a href="/2018/07/23/gan/"  title="生成对抗网络GAN">
 <strong>Next:</strong><br/> 
 <span>生成对抗网络GAN
</span>
</a>
</div>

</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span> If you are interested in Computer Vision, you can contact </span>
			<span> <a href="https://github.com/JnanLi">me.</a> </span>
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>




<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  </body>
</html>
