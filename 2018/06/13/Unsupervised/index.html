
 <!DOCTYPE HTML>
<html >
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Unsupervised CNN for Single View Depth Estimation ECCV2016 | LI jianan&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="LI jianan">
    
    <meta name="description" content="这一期的内容是无监督CNN网络在单目深度估计上的应用，也是我研究的主要方向。                                                                                                                ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.jpg">
    

  
  

    <link rel="stylesheet" href="/css/style.css">
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="LI jianan&#39;s Blog">LI jianan&#39;s Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/06/13/Unsupervised/" title="Unsupervised CNN for Single View Depth Estimation ECCV2016" itemprop="url">Unsupervised CNN for Single View Depth Estimation ECCV2016</a>
  </h1>
  <p class="article-time">
    <time datetime="2018-06-13T05:54:53.000Z" itemprop="datePublished">2018-06-13</time>
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title"></strong>
		<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#摘要"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#深度与视差的关系"><span class="toc-number">2.</span> <span class="toc-text">深度与视差的关系</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#方法"><span class="toc-number">3.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数"><span class="toc-number">3.1.</span> <span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#网络结构"><span class="toc-number">4.</span> <span class="toc-text">网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Coarse-to-fine-training-with-skip-architecture"><span class="toc-number">4.1.</span> <span class="toc-text">Coarse-to-fine training with skip architecture</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CNN模块"><span class="toc-number">5.</span> <span class="toc-text">CNN模块</span></a></li></ol>
		</div>
		
		<p>这一期的内容是无监督CNN网络在单目深度估计上的应用，也是我研究的主要方向。                                                                                                                                                                                                                                                                           </p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>使用大量人工标定的数据进行训练是大多数现有卷积神经网络的重要的弊端。本文提出一种无监督的深层卷积神经网络框架用于预测单目图像的深度，它不需要经过预训练或带标签的真实深度数据。我们通过一个类似自动编码器（autoencoder）的方式训练网络来实现这一点。在训练阶段，我们考虑使用已知相机运动信息的图像对，如双目立体图像对。我们训练卷积编码器以完成对源图像预测深度图的任务。为此，我们利用预测的深度和已知的试图间的位移，通过逆变形（inverse warp）生成目标图像，也即重建源图像。重建的光度学误差即为编码器的重建损失。这种训练数据的获取要比其它相同系统简单得多，它无需人工标定，也不需要对摄像机进行的深度传感器进行校准。结果显示，我们的网络在使用不到一半的KITTI数据集，提供了与目前最先进的基于监督学习的单视图深度估计方法相同的性能。</p>
<h1 id="深度与视差的关系"><a href="#深度与视差的关系" class="headerlink" title="深度与视差的关系"></a>深度与视差的关系</h1><p>在无监督的单目图像深度估计中，主要利用双目图像间的视差和图像深度之间的线性关系，将训练过程看作一个图像重建的问题。</p>
<p><img src="/2018/06/13/Unsupervised/dd.png" alt="dd"></p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>This approach can be interpreted in the context of convolutional autoen-coders. The task in a standard autoencoder is to encode the input with a seriesof non-linear operations to a compressed code that captures suﬃcient core infor-mation so that a decoder can reconstruct the input with minimal reconstructionerror. In our case we replace the decoder with a standard geometric image warp,based on the predicted depth map and the relative camera positions. This has two advantages: ﬁrst, the decoder in our case does not need to be learnt, sinceit is already a well-understood geometric operation; second, our reconstructionloss naturally encourages the code to be the correct depth image. </p>
<p>这种方法可以在卷积自动编码器的背景下解释。标准编码器的任务是用一系列非线性运算对输入进行编码以捕获足够的核心信息。目的是解码器能够以最小的重建误差重建输入。在我们的例子中，我们根据预测的深度图和相机位置参数用一个标准的几何图像变换来代替编码器。这有两个好处：1.我们的解码器不需要学习，因为它已经是一个很好理解的几何操作；2.重建误差能够支持我们得到正确的深度图。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数是一个可微（便于反向传播）并且和预测误差具有高度相关性。</p>
<p><img src="/2018/06/13/Unsupervised/loss.png" alt="globalloss"></p>
<p>其中，E<sub>recons</sub>为图像重建损失函数，E<sub>smooth</sub>为对视差图的平滑损失函数。</p>
<p><img src="/2018/06/13/Unsupervised/loss1.png" alt="reconsloss"><br><img src="/2018/06/13/Unsupervised/loss2.png" alt="smoothloss"></p>
<p>上式中，I<sup>i</sup>表示第i个图像对，其中i为在[1,N]范围内的整数，f为相机的焦距，B为水平放置的两个相机间的距离，d<sub>i</sub>(x)为I<sub>1</sub>对应的深度图，I<sub>w</sub>=I<sub>2</sub>(x+fB/d(x))为重建图像，γ是对深度图施加平滑的权重。</p>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>根据上述深度与视差的关系，视差图可以看作是按比例缩小的深度的逆，这个比例即为fB。</p>
<p><img src="/2018/06/13/Unsupervised/network.png" alt="network"></p>
<p><strong>part 1. 编码器，是一个传统的卷积网络，将输入的左视图(I1)映射为深度图；</strong><br><strong>part 2. 解码器，明确地将编码器的输出转换为视差图，通过扫描线(scan-line)上移动右视图(I<sub>2</sub>)的像素来获得反向变形(backward warp)图像(I<sub>w</sub>)；</strong><br><strong>part 3. 通过简单的损失函数来匹配重建的输出图像Iw和编码器输入的图像I<sub>1</sub>;</strong><br><strong>训练时，我们最小化包含视差图平滑（用以解决缝隙问题）的重建损失函数；</strong><br><strong>测试时，基于训练时给定的由fB构成的场景尺度，预测单视图的视差。</strong></p>
<p>文中提到一个backward (inverse) warp的概念，这是一个细节，下面介绍一下关于Forward warp和Backward warp的概念，如下图所示：</p>
<p><img src="/2018/06/13/Unsupervised/backward.png" alt="backward"></p>
<p>假设经过CNN网络得到的图像深度Dl=F(Il)，那么</p>
<p><strong>Forward warp 是利用D<sub>l</sub>将I<sub>l</sub>中的像素点映射到I<sub>r</sub>中，即表示为I′<sub>r</sub>=Mapping(I<sub>l</sub>,D<sub>l</sub>)；</strong><br><strong>Backward warp 是利用D<sub>l</sub>将I<sub>r</sub>中的像素映射到I<sub>l</sub>中，即表示为I′<sub>l</sub>=Mapping(I<sub>r</sub>,D<sub>l</sub>)。</strong></p>
<p>上两者的区别在于：在Forward mapping中，我们得到I’中的点可能会落在不是整数像素点的位置，这时只能通过最近原则将原图I中的像素点对应到I’中去，而在Inverse mapping中，我们从I’出发(也就是x<sub>r</sub> - D<sub>l</sub>)，去寻找相对应的原图中的点，这样能够确保I’中的每一个点都有赋值不会出现空洞，并且如果x<sub>r</sub> - D<sub>l</sub>得到的原图中的点不属于（整数）像素点，这时可以通过插值的方法求得所对应非像素点的位置。一般在这里采用双线性插值的方法，而且它在sub-pixel level是可导的。<strong>不理解为什么x<sub>r</sub>=x<sub>l1</sub>+D<sub>l1</sub>这里是加号。</strong></p>
<h2 id="Coarse-to-fine-training-with-skip-architecture"><a href="#Coarse-to-fine-training-with-skip-architecture" class="headerlink" title="Coarse-to-fine training with skip architecture"></a>Coarse-to-fine training with skip architecture</h2><p>为了计算上文设计的损失函数的梯度，我们需要用泰勒表达式对I<sub>2</sub>线性化：</p>
<p><img src="/2018/06/13/Unsupervised/taylor.png" alt="taylor"></p>
<p>I<sub>2h</sub>表示第n次迭代在当前视差D<sup>n-1</sup>的水平方向上的梯度。</p>
<p>Thankfully recent fully convolutional architecture with upsampling proposed in [18] is appropriate choice simulate coarse to ﬁne warping for our system. As depicted in Figure 2, given a networks which predict M×N depth map we can use a simple bilinear upsampling ﬁlter, to initialize upscaled depths (to get 2M×2N depth maps) keeping the other network parametersﬁxed. It has been shown that the ﬁner details of the images are captured in the previous layers of CNN and fusing back such information is helpful for reﬁning a coarse CNN prediction. We use a simple 1×1 convolution with the ﬁlter and bias both initialized to zero and simply use the convolved image output to upscaled coarse depth.</p>
<p>好在[18]中提出的带上采样的全卷积网络结构对我们来说是合适的选择，可以模拟coarse to fine的过程。如图2所示，给定一个M×N的深度预测网络，我们可以使用上采样滤波器来初始化放大之后的深度，并保持其他网络参数不变。图像的细节被捕获在CNN的前几层中，融合这些信息有助于提炼之前的coarse预测。</p>
<p><img src="/2018/06/13/Unsupervised/ctf.JPG" alt="Coarse to fine"></p>
<h1 id="CNN模块"><a href="#CNN模块" class="headerlink" title="CNN模块"></a>CNN模块</h1><p>本文的CNN采用了全卷积网络(FCN)以达到coarse to fine的预测。</p>
<p><img src="/2018/06/13/Unsupervised/CNN.png" alt="CNN"></p>
<p>图中：C（红色）- 卷积、P（黄）- pooling、N（紫）- 归一化、F（绿）- FCN、D（蓝）- 上采样</p>
  
	</div>
		<footer class="article-footer clearfix">




<div class="article-share" id="share">

  <div data-url="https://www.lijn.tech/2018/06/13/Unsupervised/" data-title="Unsupervised CNN for Single View Depth Estimation ECCV2016 | LI jianan&#39;s Blog" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/06/14/ccbdai/" title="云计算、大数据和人工智能">
  <strong>Previous:</strong><br/>
  <span>
  云计算、大数据和人工智能</span>
</a>
</div>


<div class="next">
<a href="/2018/06/13/DeepStereo/"  title="Deep Stereo Learning to Predict New Views CVPR2016">
 <strong>Next:</strong><br/> 
 <span>Deep Stereo Learning to Predict New Views CVPR2016
</span>
</a>
</div>

</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span> If you are interested in Computer Vision, you can contact </span>
			<span> <a href="https://github.com/JnanLi">me.</a> </span>
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>




<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  </body>
</html>
