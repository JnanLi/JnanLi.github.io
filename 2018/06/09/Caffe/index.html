
 <!DOCTYPE HTML>
<html >
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Caffe | LI jianan&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="LI jianan">
    
    <meta name="description" content="看视频学Caffe，做个简要的笔记记录一下。                                                                                                                                ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.jpg">
    

  
  

    <link rel="stylesheet" href="/css/style.css">
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="LI jianan&#39;s Blog">LI jianan&#39;s Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/06/09/Caffe/" title="Caffe" itemprop="url">Caffe</a>
  </h1>
  <p class="article-time">
    <time datetime="2018-06-09T06:13:40.000Z" itemprop="datePublished">2018-06-09</time>
  </p>
</header>
	<div class="article-content">
		
		
		<p>看视频学Caffe，做个简要的笔记记录一下。                                                                                                                                                                                                                                                                                                     </p>
<h1 id="1-网络配置"><a href="#1-网络配置" class="headerlink" title="1. 网络配置"></a>1. 网络配置</h1><h2 id="1-1-Caffe简介"><a href="#1-1-Caffe简介" class="headerlink" title="1.1 Caffe简介"></a>1.1 Caffe简介</h2><table>
<thead>
<tr>
<th style="text-align:left">Blob</th>
<th style="text-align:center">Layer</th>
<th style="text-align:center">Net</th>
<th style="text-align:right">Solver</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">存储数据及其衍生物</td>
<td style="text-align:center">传输bottom blobs到top blobs</td>
<td style="text-align:center">由许多Layers组成，计算正向传播和反向传播的梯度</td>
<td style="text-align:right">更新权重</td>
</tr>
</tbody>
</table>
<p><strong>Blob封装了运行时的数据信息，提供了CPU和GPU的同步，从数学上来说，Blob就是一个N维数组。它是caffe的基本操作单元，就像matlab中以矩阵为基本操作对象一样。比如Blob可以表示为NCHW这样一个4维数组。</strong></p>
<h2 id="1-2-Caffe-Model-Zoo"><a href="#1-2-Caffe-Model-Zoo" class="headerlink" title="1.2 Caffe Model Zoo"></a>1.2 Caffe Model Zoo</h2><p>AlexNet VGG GoogleNet ResNet …</p>
<h2 id="1-3-数据层详解"><a href="#1-3-数据层详解" class="headerlink" title="1.3 数据层详解"></a>1.3 数据层详解</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">    <span class="string">'''表示该层的名字，可随意取'''</span></span><br><span class="line">    name: <span class="string">"cifar"</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    层类型，如果是Data，表示数据来源于LevelDB或LMDB。</span></span><br><span class="line"><span class="string">    根据数据来源的不同，数据层的类型也不同。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    type: <span class="string">"Data"</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    每一层用bottom来输入数据，用top来输出数据。</span></span><br><span class="line"><span class="string">    如果只有top没有bottom，表示此层只有输出没有输入。</span></span><br><span class="line"><span class="string">    在数据层中，至少有一个命名为data的top，</span></span><br><span class="line"><span class="string">    (top, lable)配对是分类模型所必需的。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    top: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"label"</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    一般在训练的时候和测试的时候，模型的层是不一样的。</span></span><br><span class="line"><span class="string">    该层属于训练阶段的层，还是属于测试阶段的层，需要用include来指定。</span></span><br><span class="line"><span class="string">    如果没有include参数，表示该层即在训练模型中，又在测试模型中。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    include &#123;</span><br><span class="line">        phase: TRAIN</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">'''数据的预处理，将数据变换到定义的范围内。'''</span></span><br><span class="line">    transform_param &#123;</span><br><span class="line">        <span class="string">'''用一个配置文件来进行均值处理'''</span></span><br><span class="line">        mean_file: <span class="string">"examples/cifar10/mean.binaryproto"</span></span><br><span class="line">        transform_param &#123;</span><br><span class="line">            <span class="string">'''实际上就是1/255，即将输入数据由0-255归一化到0-1。'''</span></span><br><span class="line">            scale: <span class="number">0.00390625</span></span><br><span class="line">            <span class="string">'''1表示开启镜像，0表示关闭，也可以用true和false来表示'''</span></span><br><span class="line">            mirror: true</span><br><span class="line">            <span class="string">'''剪裁一个227*227的图像块，在训练阶段随机剪裁，在测试阶段从中间剪裁'''</span></span><br><span class="line">            crop_size: <span class="number">227</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    data_param &#123;</span><br><span class="line">        <span class="string">'''数据库来源'''</span></span><br><span class="line">        source: <span class="string">"examples/cifar10/cifar10_train_lmdb"</span></span><br><span class="line">        batch_size: <span class="number">64</span></span><br><span class="line">        <span class="string">'''选用数据的名称'''</span></span><br><span class="line">        backend: LMDB</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-3-1-数据来自于数据库-LevelDB、LMDB"><a href="#1-3-1-数据来自于数据库-LevelDB、LMDB" class="headerlink" title="1.3.1 数据来自于数据库(LevelDB、LMDB)"></a>1.3.1 数据来自于数据库(LevelDB、LMDB)</h3><p><strong>层类型：Data<br>必须设置的参数</strong></p>
<ul>
<li>source：包含数据库的目录名称</li>
<li>batch_size：<br><strong>可选的参数：</strong></li>
<li>rand_skip：？</li>
<li>backend：选择是采用LevelDB还是LMDB，默认是LevelDB<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''使用LMDB源，一般做分类用'''</span></span><br><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"mnist"</span></span><br><span class="line">    type: <span class="string">"Data"</span></span><br><span class="line">    top: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"label"</span></span><br><span class="line">    include &#123;</span><br><span class="line">        phase: TRAIN</span><br><span class="line">    &#125;</span><br><span class="line">    transform_param &#123;</span><br><span class="line">        scale: <span class="number">0.00390625</span></span><br><span class="line">    &#125;</span><br><span class="line">    data_param &#123;</span><br><span class="line">        source: <span class="string">"examples/mnist/mnist_train_lmdb"</span></span><br><span class="line">        batch_size: <span class="number">64</span></span><br><span class="line">        backend: LMDB</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="1-3-2-数据来自于内存"><a href="#1-3-2-数据来自于内存" class="headerlink" title="1.3.2 数据来自于内存"></a>1.3.2 数据来自于内存</h2><p><strong>层类型：MemoryData<br>必须设置的参数：</strong></p>
<ul>
<li>batch_size：</li>
<li>channels：通道数？为什么前面那个不用设置</li>
<li>height：</li>
<li>width：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"memo"</span></span><br><span class="line">    type: <span class="string">"MemoryData"</span></span><br><span class="line">    top: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"label"</span></span><br><span class="line">    memory_data_param&#123;</span><br><span class="line">        batch_size: <span class="number">10</span></span><br><span class="line">        height: <span class="number">224</span></span><br><span class="line">        width: <span class="number">224</span></span><br><span class="line">        channels: <span class="number">3</span></span><br><span class="line">    &#125;</span><br><span class="line">    transform_param&#123;</span><br><span class="line">        scale: <span class="number">0.0078125</span></span><br><span class="line">        mean_file: <span class="string">"mean.proto"</span></span><br><span class="line">        mirror: true</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>###1.3.3 数据来自于HDF5<br><strong>层类型：HDF5Data<br>必须设置的参数：</strong></p>
<ul>
<li>source：读取的文件名称</li>
<li>batch_size：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''使用HDF5数据源，一般做回归用'''</span></span><br><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"data"</span></span><br><span class="line">    type: <span class="string">"HDF5Data"</span></span><br><span class="line">    top: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"label"</span></span><br><span class="line">    hdf5_data_param &#123;</span><br><span class="line">        source: <span class="string">"examples/hdf5_classification/data/train.txt"</span></span><br><span class="line">        batch_size: <span class="number">10</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-3-4-数据源直接来源于图片"><a href="#1-3-4-数据源直接来源于图片" class="headerlink" title="1.3.4 数据源直接来源于图片"></a>1.3.4 数据源直接来源于图片</h3><p><strong>层类型：ImageData<br>必须设置的参数：</strong></p>
<ul>
<li>source：一个文本文件的名字，每一行给定一个图片文件的名称和标签</li>
<li>batch_size：<br><strong>可选参数：</strong></li>
<li>rand_skip：</li>
<li>shuffle：随机打乱顺序，默认为false</li>
<li>new_height，new_width：如果设置，就对图像resize<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"data"</span></span><br><span class="line">    type: <span class="string">"ImageData"</span></span><br><span class="line">    top: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"label"</span></span><br><span class="line">    transform_param &#123;</span><br><span class="line">        mirror: false</span><br><span class="line">        crop_size: <span class="number">227</span></span><br><span class="line">        mean_file: <span class="string">"data/ilsvrc12/imagenet_mean.binaryproto"</span></span><br><span class="line">    &#125;</span><br><span class="line">    image_data_param &#123;</span><br><span class="line">        source: <span class="string">"examples/_temp/file_list.txt"</span></span><br><span class="line">        batch_size: <span class="number">64</span></span><br><span class="line">        new_height: <span class="number">256</span></span><br><span class="line">        new_width: <span class="number">256</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>数据是每个模型的最底层，是模型的入口，不仅提供数据的输入，也提供数据从Blobs转换成别的格式进行保存输出。通常<strong>数据的预处理(如减去均值，放大缩小，裁剪和镜像等)，也在这一层设置参数实现。</strong></p>
<h2 id="1-4-卷积层"><a href="#1-4-卷积层" class="headerlink" title="1.4 卷积层"></a>1.4 卷积层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"conv1"</span></span><br><span class="line">    type: <span class="string">"Convolution"</span></span><br><span class="line">    bottom: <span class="string">"data"</span></span><br><span class="line">    top: <span class="string">"conv1"</span></span><br><span class="line">    param &#123;</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        学习率的系数，最终的学习率是这个数乘以slover.prototxt配置文件中的base_lr。</span></span><br><span class="line"><span class="string">        如果有两个lr_mult，则第一个表示权值的学习率，第二个表示偏置项的学习率。</span></span><br><span class="line"><span class="string">        一般偏置项的学习率是权值学习率的两倍</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        lr_mult = <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    param &#123;</span><br><span class="line">        lr_mult = <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    convolution_param &#123;</span><br><span class="line">        num_output: <span class="number">20</span></span><br><span class="line">        kernel_size: <span class="number">5</span></span><br><span class="line">        <span class="string">'''默认为1'''</span></span><br><span class="line">        stride: <span class="number">1</span></span><br><span class="line">        <span class="string">'''默认为0'''</span></span><br><span class="line">        pad: <span class="number">0</span></span><br><span class="line">        <span class="string">'''权值初始化。默认为"constant"，值全为0，也可以设置为guassian'''</span></span><br><span class="line">        weight_filler &#123;</span><br><span class="line">            type: <span class="string">"xavier"</span></span><br><span class="line">        &#125;</span><br><span class="line">        bias_filler &#123;</span><br><span class="line">            type: <span class="string">"constant"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>还有</p>
<ul>
<li>bias_term：是否开启偏置项，默认为true</li>
<li>group：分组，默认为1组。如果大于1，我们限制卷积的连接操作在一个子集内。如果我们根据图像的通道来分组，那么第i个输出分组只能与第i个输入分组进行连接。<br>上述都是必须设置的参数，有些有默认值的特殊情况。</li>
</ul>
<h2 id="1-5-池化层"><a href="#1-5-池化层" class="headerlink" title="1.5 池化层"></a>1.5 池化层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	name: <span class="string">"pool1"</span></span><br><span class="line">	type: <span class="string">"Pooling"</span></span><br><span class="line">	bottom: <span class="string">"conv1"</span></span><br><span class="line">	top: <span class="string">"pool1"</span></span><br><span class="line">	pooling_param &#123;</span><br><span class="line">        <span class="string">'''MAX AVE STOCHASTIC'''</span></span><br><span class="line">	    pool: MAX</span><br><span class="line">	    kernel_size: <span class="number">3</span></span><br><span class="line">	    <span class="string">'''默认为1，一般设置为2，即互相不重叠(当k=2)'''</span></span><br><span class="line">        stride: <span class="number">2</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>还有</p>
<ul>
<li>pad：默认为0</li>
</ul>
<h2 id="1-6-Local-Response-Normalization-LRN-层"><a href="#1-6-Local-Response-Normalization-LRN-层" class="headerlink" title="1.6 Local Response Normalization(LRN)层"></a>1.6 Local Response Normalization(LRN)层</h2><p>此层是对输入的局部区域进行归一化，达到”侧抑制”的效果，<br><strong>层类型：LRN<br>参数均有默认值：</strong></p>
<ul>
<li>norm_region: 默认为ACROSS_CHANNELS。有两个选择，ACROSS_CHANNELS表示在相邻的通道间求和归一化。WITHIN_CHANNEL表示在一个通道内部特定的区域内进行求和归一化。与前面的local_size参数对应。</li>
<li>local_size：默认为5，如果是跨通道LRN，则表示求和的通道数；如果是在通道内LRN，则表示求和的正方形区域长度。</li>
<li>alpha：默认为1，归一化公式中的参数</li>
<li>beta：默认为5，归一化公式中的参数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">layers&#123;</span><br><span class="line">    name: <span class="string">"nrom1"</span></span><br><span class="line">    type: <span class="string">"LRN"</span></span><br><span class="line">    bottom: <span class="string">"pool1"</span></span><br><span class="line">    top: <span class="string">"nrom1"</span></span><br><span class="line">    lrn_param&#123;</span><br><span class="line">        alpha: <span class="number">0.01</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="1-7-激活层"><a href="#1-7-激活层" class="headerlink" title="1.7 激活层"></a>1.7 激活层</h2><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p><img src="/2018/06/09/Caffe/sig.PNG" alt=""><br><img src="/2018/06/09/Caffe/sig1.PNG" alt=""><br><strong>层类型: Sigmoid</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layers&#123;</span><br><span class="line">    name: <span class="string">"sigmoid1"</span></span><br><span class="line">    type: <span class="string">"Sigmoid"</span></span><br><span class="line">    bottom: <span class="string">"pool1"</span></span><br><span class="line">    top: <span class="string">"sigmoid1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><strong>f(x)=max(x,0)<br>层类型： “ReLU”</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	name: <span class="string">"relu1"</span></span><br><span class="line">	type: <span class="string">"ReLU"</span></span><br><span class="line">	bottom: <span class="string">"pool1"</span></span><br><span class="line">	top: <span class="string">"relu1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p><img src="/2018/06/09/Caffe/tanh.PNG" alt=""><br><strong>[-1, 1]<br>层类型： “TanH”</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layers&#123;</span><br><span class="line">    name: <span class="string">"tanh1"</span></span><br><span class="line">    type: <span class="string">"TanH"</span></span><br><span class="line">    bottom: <span class="string">"pool1"</span></span><br><span class="line">    top: <span class="string">"tanh1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="1-8-全连接层"><a href="#1-8-全连接层" class="headerlink" title="1.8 全连接层"></a>1.8 全连接层</h2><p><strong>全连接层也是一种卷积层，只是它的卷积核大小和特征图大小一致。<br>层类型： “InnerProduct”，<br>参数和卷积层差不多一样。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	name: <span class="string">"ip1"</span></span><br><span class="line">    type: <span class="string">"InnerProduct"</span></span><br><span class="line">    bottom: <span class="string">"pool2"</span></span><br><span class="line">    top: <span class="string">"ip1"</span></span><br><span class="line">    param &#123;</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        学习率的系数，最终的学习率是这个数乘以slover.prototxt配置文件中的base_lr。</span></span><br><span class="line"><span class="string">        如果有两个lr_mult，则第一个表示权值的学习率，第二个表示偏置项的学习率。</span></span><br><span class="line"><span class="string">        一般偏置项的学习率是权值学习率的两倍</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        lr_mult: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    param &#123;</span><br><span class="line">        lr_mult: <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    inner_product_param &#123;</span><br><span class="line">        num_output: <span class="number">500</span></span><br><span class="line">        weight_filler &#123;</span><br><span class="line">            type: <span class="string">"xavier"</span></span><br><span class="line">        &#125;</span><br><span class="line">        bias_filler &#123;</span><br><span class="line">            type: <span class="string">"constant"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="1-9-准确率accuracy"><a href="#1-9-准确率accuracy" class="headerlink" title="1.9 准确率accuracy"></a>1.9 准确率accuracy</h2><p><strong>只有test阶段才有，因此要加入include参数。<br>层类型： “Accuracy”</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"accuracy"</span></span><br><span class="line">    type: <span class="string">"Accuracy"</span></span><br><span class="line">    bottom: <span class="string">"ip2"</span></span><br><span class="line">    bottom: <span class="string">"label"</span></span><br><span class="line">    top: <span class="string">"accuracy"</span></span><br><span class="line">    include &#123;</span><br><span class="line">        phase: TEST</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="1-10-softmax-loss"><a href="#1-10-softmax-loss" class="headerlink" title="1.10 softmax-loss"></a>1.10 softmax-loss</h2><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p><img src="/2018/06/09/Caffe/soft.jpg" alt=""><br>假设你的全连接层输出[1,2,3]，那么经过softmax层后就会得到[0.09,0.24,0.67]，这三个数字表示这个样本属于第1,2,3类的概率分别是0.09,0.24,0.67。</p>
<h3 id="softmax-loss"><a href="#softmax-loss" class="headerlink" title="softmax loss"></a>softmax loss</h3><p><img src="/2018/06/09/Caffe/soft1.jpg" alt=""><br>来举个例子吧。假设一个5分类问题，然后一个样本I的标签y=[0,0,0,1,0]，也就是说样本I的真实标签是4，假设模型预测的结果概率（softmax的输出）p=[0.1,0.15,0.05,<strong>0.6</strong>,0.1]，可以看出这个预测是对的，那么对应的损失L=-log(0.6)，也就是当这个样本经过这样的网络参数产生这样的预测p时，它的损失是-log(0.6)。那么假设p=[0.15,0.2,0.4,<strong>0.1</strong>,0.15]，这个预测结果就很离谱了，因为真实标签是4，而你觉得这个样本是4的概率只有0.1（远不如其他概率高，如果是在测试阶段，那么模型就会预测该样本属于类别3），对应损失L=-log(0.1)。那么假设p=[0.05,0.15,0.4,<strong>0.3</strong>,0.1]，这个预测结果虽然也错了，但是没有前面那个那么离谱，对应的损失L=-log(0.3)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''softmax-loss layer'''</span></span><br><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"loss"</span></span><br><span class="line">    type: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">    bottom: <span class="string">"ip1"</span></span><br><span class="line">    bottom: <span class="string">"label"</span></span><br><span class="line">    top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="string">'''softmax layer'''</span></span><br><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"prob"</span></span><br><span class="line">    type: <span class="string">"Softmax"</span></span><br><span class="line">    bottom: <span class="string">"cls3_fc"</span></span><br><span class="line">    top: <span class="string">"prob"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="1-11-reshape层"><a href="#1-11-reshape层" class="headerlink" title="1.11 reshape层"></a>1.11 reshape层</h2><p><strong>在不改变数据的情况下，改变输入的维度。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"reshape"</span></span><br><span class="line">    type: <span class="string">"Reshape"</span></span><br><span class="line">    bottom: <span class="string">"input"</span></span><br><span class="line">    top: <span class="string">"output"</span></span><br><span class="line">    reshape_param &#123;</span><br><span class="line">        shape &#123;</span><br><span class="line">            dim: <span class="number">0</span></span><br><span class="line">            dim: <span class="number">0</span></span><br><span class="line">            dim: <span class="number">14</span></span><br><span class="line">            dim: <span class="number">-1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>有一个可选的参数组shape，用于指定blob数据的各维度的值（blob是一个四维的数据：32×3×28×28）。<br>dim: 0 表示维度不变，即输入和输出是相同的维度<br>dim: 14 将原来的维度变成14<br>dim： -1 表示由系统自动计算维度，数据的总量不变，系统会根据blob数据的其它三维来自动计算当前维的维度值。<br>输出数据为： 32×3×14×56</strong></p>
<h2 id="1-12-Dropout"><a href="#1-12-Dropout" class="headerlink" title="1.12 Dropout"></a>1.12 Dropout</h2><p><strong>Dropout是一个防止过拟合的trick。可以随机让网络某些隐含层节点的权重不工作。<br>只需要设置一个dropout_ratio就可以了。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">    name: <span class="string">"drop7"</span></span><br><span class="line">    type: <span class="string">"Dropout"</span></span><br><span class="line">    bottom: <span class="string">"fc7_conv"</span></span><br><span class="line">    top: <span class="string">"fc7_conv"</span></span><br><span class="line">    dropout_param &#123;</span><br><span class="line">        dropout_ratio: <span class="number">0.5</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="2-Solver"><a href="#2-Solver" class="headerlink" title="2. Solver"></a>2. Solver</h1><p><strong>Solver是caffe的核心，协调整个模型的运作。caffe程序必带的一个参数就是solver配置文件，运行的代码一般为：</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">caffe train --solver = *_solver.prototxt</span><br></pre></td></tr></table></figure></p>
<p><strong>Solver的主要作用就是交替调用前向和后向算法来更新参数。</strong><br>Caffe提供了六种优化算法来求解最优参数，在solver配置文件中，通过设置type类型来选择。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Algorithm</th>
<th style="text-align:center">type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Stochastic Gradient Descent</td>
<td style="text-align:center">SGD</td>
</tr>
<tr>
<td style="text-align:center">AdaDelta</td>
<td style="text-align:center">AdaDelta</td>
</tr>
<tr>
<td style="text-align:center">Adaptive Gradient</td>
<td style="text-align:center">AdaGrad</td>
</tr>
<tr>
<td style="text-align:center">Adam</td>
<td style="text-align:center">Adam</td>
</tr>
<tr>
<td style="text-align:center">RMSprop</td>
<td style="text-align:center">RMSprop</td>
</tr>
</tbody>
</table>
<p>Solver的流程：</p>
<ul>
<li>设计好需要优化的对象，以及用于学习的训练网络和用于评估的测试网络。(通过调用另外一个配置文件protocol进行)</li>
<li>通过forward和backward交替优化更新参数</li>
<li>定期地评价测试网络。(可设定多少次训练之后，进行一次测试)</li>
<li>在优化过程中显示模型和Solver状态</li>
</ul>
<p>在每一次的迭代中，Solver做了这几步工作：</p>
<ul>
<li>调用forward算法来计算最终的输出值，以及对应的loss</li>
<li>调用backward算法来计算每层的梯度</li>
<li>根据选用的Solver方法，利用梯度进行参数更新</li>
<li>记录并保存每次迭代的学习率、快照以及对应的状态</li>
</ul>
<p>接下来看一个实例，并对每一项做出解释<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">net: <span class="string">"examples/mnist/lenet_train_test.prototxt"</span></span><br><span class="line">test_iter: <span class="number">100</span></span><br><span class="line">test_interval: <span class="number">1000</span></span><br><span class="line">base_lr: <span class="number">0.01</span></span><br><span class="line">momentum: <span class="number">0.9</span></span><br><span class="line">type: SGD</span><br><span class="line">weight_decay: <span class="number">0.0005</span></span><br><span class="line">lr_policy: <span class="string">"inv"</span></span><br><span class="line">gamma: <span class="number">0.0001</span></span><br><span class="line">power: <span class="number">0.75</span></span><br><span class="line">display: <span class="number">500</span></span><br><span class="line">max_iter: <span class="number">20000</span></span><br><span class="line">snapshot: <span class="number">5000</span></span><br><span class="line">snapshot_prefix: <span class="string">"examples/mnist/lenet"</span></span><br><span class="line">solver_mode: CPU</span><br></pre></td></tr></table></figure></p>
<h2 id="2-1-网络配置文件位置"><a href="#2-1-网络配置文件位置" class="headerlink" title="2.1 网络配置文件位置"></a>2.1 网络配置文件位置</h2><p><strong>net: “examples/mnist/lenet_train_test.prototxt”</strong><br>设置深度网络模型。每一个模型就是一个net，需要在一个专门的配置文件中对net进行配置，每个net由许多的layer所组成。<br><strong>注意：文件的路径要从caffe的根目录开始，其它的所有配置都是这样。</strong><br>也可以分别设置为train_net和test_net：<br>train_net: “examples/mnist/lenet_auto_train.prototxt”<br>test_net: “examples/mnist/lenet_auto_test.prototxt”</p>
<h2 id="2-2"><a href="#2-2" class="headerlink" title="2.2"></a>2.2</h2><p><strong>test_iter: 100</strong><br>这个要与test layer中的batch_size结合起来理解。mnist数据中测试样本总数为10000，一次性执行全部数据效率很低，因此我们将测试数据分成几个批次来执行，每个批次的数量就是batch_size。假设我们设置batch_size为100，则需要迭代100次才能将10000个数据全部执行完。因此test_iter设置为100。执行完一次全部数据，称之为一个epoch。</p>
<h2 id="2-3-测试间隔"><a href="#2-3-测试间隔" class="headerlink" title="2.3 测试间隔"></a>2.3 测试间隔</h2><p><strong>test_interval: 1000</strong><br>也就是每训练1000次，才进行一次测试。</p>
<h2 id="2-4-学习率"><a href="#2-4-学习率" class="headerlink" title="2.4 学习率"></a>2.4 学习率</h2><p>base_lr： 0.01<br>lr_policy： “inv”<br>gamma: 0.0001<br>power: 0.75</p>
<p>第一行是基础学习率。lr_policy可以设置为下面这些值</p>
<table>
<thead>
<tr>
<th style="text-align:center">policy</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">fixed</td>
<td style="text-align:center">保持base_lr不变</td>
</tr>
<tr>
<td style="text-align:center">step</td>
<td style="text-align:center">还需要设置stepsize，base_lr * gamma ^ (floor(iter / stepsize))</td>
</tr>
<tr>
<td style="text-align:center">exp</td>
<td style="text-align:center">base_lr * gamma ^ iter</td>
</tr>
<tr>
<td style="text-align:center">inv</td>
<td style="text-align:center">还需要设置power，base_lr <em> (1 + gamma </em> iter) ^ (-power) 常用</td>
</tr>
<tr>
<td style="text-align:center">multistep</td>
<td style="text-align:center">还需要设置stepvalue。</td>
</tr>
<tr>
<td style="text-align:center">poly</td>
<td style="text-align:center">略</td>
</tr>
<tr>
<td style="text-align:center">sigmoid</td>
<td style="text-align:center">略</td>
</tr>
</tbody>
</table>
<h2 id="2-6-动量"><a href="#2-6-动量" class="headerlink" title="2.6 动量"></a>2.6 动量</h2><p>momentum： 0.9</p>
<h2 id="2-7-每训练多少次，显示一下"><a href="#2-7-每训练多少次，显示一下" class="headerlink" title="2.7 每训练多少次，显示一下"></a>2.7 每训练多少次，显示一下</h2><p>display： 500<br><strong>每训练100次，在屏幕上显示一次。如果设置为0，则不显示。</strong></p>
<h2 id="2-8-最大迭代次数"><a href="#2-8-最大迭代次数" class="headerlink" title="2.8 最大迭代次数"></a>2.8 最大迭代次数</h2><p>max_iter: 100000<br><strong>这个数设置太小，会导致没有收敛，精确度很低。设置太大，会导致震荡，浪费时间。</strong></p>
<h2 id="2-9-快照"><a href="#2-9-快照" class="headerlink" title="2.9 快照"></a>2.9 快照</h2><p>snapshot： 1000<br>snapshot_prefix: “examples/mnist/lenet”</p>
<p><strong>将训练出来的model和solver状态进行保存，snapshot用于设置训练多少次后进行保存，默认为0，不保存。snapshot_prefix设置保存路径。<br>还可以设置snapshot_diff，是否保存梯度值，默认为false,不保存。<br>也可以设置snapshot_format，保存的类型。有两种选择：HDF5 和BINARYPROTO ，默认为BINARYPROTO</strong></p>
<h2 id="2-10-运行模式"><a href="#2-10-运行模式" class="headerlink" title="2.10 运行模式"></a>2.10 运行模式</h2><p>solver_mode: CPU<br><strong>设置运行模式。默认为GPU,如果你没有GPU,则需要改成CPU,否则会出错。</strong></p>
<h2 id="2-11-SGD"><a href="#2-11-SGD" class="headerlink" title="2.11 SGD"></a>2.11 SGD</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">base_lr: <span class="number">0.01</span> </span><br><span class="line">lr_policy: <span class="string">"step"</span></span><br><span class="line">gamma: <span class="number">0.1</span>   </span><br><span class="line">stepsize: <span class="number">1000</span>  </span><br><span class="line">max_iter: <span class="number">3500</span> </span><br><span class="line">momentum: <span class="number">0.9</span></span><br></pre></td></tr></table></figure>
<p>即前1000次迭代，学习率为0.01; 第1001-2000次迭代，学习率为0.001; 第2001-3000次迭代，学习率为10-4，第3001-3500次迭代，学习率为10-5。</p>
<h2 id="2-12-AdaDelta"><a href="#2-12-AdaDelta" class="headerlink" title="2.12 AdaDelta"></a>2.12 AdaDelta</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">net: <span class="string">"examples/mnist/lenet_train_test.prototxt"</span></span><br><span class="line">test_iter: <span class="number">100</span></span><br><span class="line">test_interval: <span class="number">500</span></span><br><span class="line">base_lr: <span class="number">1.0</span></span><br><span class="line">lr_policy: <span class="string">"fixed"</span></span><br><span class="line">momentum: <span class="number">0.95</span></span><br><span class="line">weight_decay: <span class="number">0.0005</span></span><br><span class="line">display: <span class="number">100</span></span><br><span class="line">max_iter: <span class="number">10000</span></span><br><span class="line">snapshot: <span class="number">5000</span></span><br><span class="line">snapshot_prefix: <span class="string">"examples/mnist/lenet_adadelta"</span></span><br><span class="line">solver_mode: GPU</span><br><span class="line">type: <span class="string">"AdaDelta"</span></span><br><span class="line">delta: <span class="number">1e-6</span></span><br></pre></td></tr></table></figure>
<p><strong>从最后两行可看出，设置solver type为Adadelta时，需要设置delta的值。</strong></p>
<h2 id="2-13-AdaGrad"><a href="#2-13-AdaGrad" class="headerlink" title="2.13 AdaGrad"></a>2.13 AdaGrad</h2><p>自适应梯度（adaptive gradient）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">net: <span class="string">"examples/mnist/mnist_autoencoder.prototxt"</span></span><br><span class="line">test_state: &#123; stage: <span class="string">'test-on-train'</span> &#125;</span><br><span class="line">test_iter: <span class="number">500</span></span><br><span class="line">test_state: &#123; stage: <span class="string">'test-on-test'</span> &#125;</span><br><span class="line">test_iter: <span class="number">100</span></span><br><span class="line">test_interval: <span class="number">500</span></span><br><span class="line">test_compute_loss: true</span><br><span class="line">base_lr: <span class="number">0.01</span></span><br><span class="line">lr_policy: <span class="string">"fixed"</span></span><br><span class="line">display: <span class="number">100</span></span><br><span class="line">max_iter: <span class="number">65000</span></span><br><span class="line">weight_decay: <span class="number">0.0005</span></span><br><span class="line">snapshot: <span class="number">10000</span></span><br><span class="line">snapshot_prefix: <span class="string">"examples/mnist/mnist_autoencoder_adagrad_train"</span></span><br><span class="line"><span class="comment"># solver mode: CPU or GPU</span></span><br><span class="line">solver_mode: GPU</span><br><span class="line">type: <span class="string">"AdaGrad"</span></span><br></pre></td></tr></table></figure></p>
<h1 id="3-制作LMDB数据源"><a href="#3-制作LMDB数据源" class="headerlink" title="3. 制作LMDB数据源"></a>3. 制作LMDB数据源</h1><p>假设你有一个文件夹face_detect。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Example=/home/path/to/face_detect</span><br><span class="line"></span><br><span class="line">Data=/home/path/to/face_detect</span><br><span class="line">Tools=/home/path/to/caffe/build/tools</span><br><span class="line"></span><br><span class="line">TRAIN_DATA_ROOT=/home/path/to/face_detect/train/</span><br><span class="line">VAL_DATA_ROOT=/home/path/to/face_detect/val/</span><br></pre></td></tr></table></figure>
<p>如果RESIZE=true，修改RESIZE_HEIGHT和RESIZE_WIDTH。<br>运行sh文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh face-lmdb.sh</span><br></pre></td></tr></table></figure>
<p>还有一个要修改的地方：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Creating train lmdb..."</span></span><br><span class="line"></span><br><span class="line">GLOG_logtostderr=1 <span class="variable">$TOOLS</span>/convert_image \</span><br><span class="line">    --resize_height=<span class="variable">$RESIZE_HEIGHT</span> \</span><br><span class="line">    --resize_width=<span class="variable">$RESIZE_WIDTH</span> \</span><br><span class="line">    --shuffle \</span><br><span class="line">    <span class="variable">$TRAIN_DATA_ROOT</span> \</span><br><span class="line">    <span class="variable">$DATA</span>/train.txt \</span><br><span class="line">    <span class="variable">$EXAMPLE</span>/face_train_lmdb</span><br></pre></td></tr></table></figure>
<p>上面的train.txt可以自己跑个python程序来制作，face_train_lmdb是存放lmdb的文件夹。</p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/Caffe/">Caffe</a>
  </div>




<div class="article-share" id="share">

  <div data-url="https://www.lijn.tech/2018/06/09/Caffe/" data-title="Caffe | LI jianan&#39;s Blog" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/06/12/人类简史/" title="读 人类简史——从动物到上帝">
  <strong>Previous:</strong><br/>
  <span>
  读 人类简史——从动物到上帝</span>
</a>
</div>


</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span> If you are interested in Computer Vision, you can contact </span>
			<span> <a href="https://github.com/JnanLi">me.</a> </span>
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>




<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  </body>
</html>
