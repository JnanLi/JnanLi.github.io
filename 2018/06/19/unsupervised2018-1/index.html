
 <!DOCTYPE HTML>
<html >
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction CVPR_2018 | LI jianan&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="LI jianan">
    
    <meta name="description" content="这一期介绍2018年关于无监督深度估计的第一篇论文。                                                                                                                            ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.jpg">
    

  
  

    <link rel="stylesheet" href="/css/style.css">
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="LI jianan&#39;s Blog">LI jianan&#39;s Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/06/19/unsupervised2018-1/" title="Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction CVPR_2018" itemprop="url">Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction CVPR_2018</a>
  </h1>
  <p class="article-time">
    <time datetime="2018-06-19T12:34:52.000Z" itemprop="datePublished">2018-06-19</time>
  </p>
</header>
	<div class="article-content">
		
		
		<p>这一期介绍2018年关于无监督深度估计的第一篇论文。                                                                                                                                                                                                                                                                                                  </p>
<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h1><p>尽管基于学习的方法在单视图深度估计和视觉测程法上显示出有希望的结果，但大多数现有方法都以有监督的方式来处理任务。最近的单视图深度估计方法通过<strong>最小化光度学误差</strong>来学习而非全监督方式。在本文中，我们探索使用立体序列对来学习深度和视觉量距。立体序列对的使用使得能够使用空间（左右）和时间（前后）光度学误差，并将场景深度和相机运动限制在常见的真实世界中。测试时，我们的框架能从单目序列估计场景单视图深度和双视图测距。我们还展示了如何通过考虑深度特征的变形来改进标准光度学误差。我们通过广泛的实验表明：（i）联合训练单视图深度和视觉测距提高了深度预测精度，因为附加的深度限制，并为视觉测距取得了良好的结果。（ii）深度特征的warping loss改善了单视图深度估计和视觉测距的光度学warp loss。</p>
<h1 id="2-介绍"><a href="#2-介绍" class="headerlink" title="2. 介绍"></a>2. 介绍</h1><p><img src="/2018/06/19/unsupervised2018-1/train.jpg" alt="train"></p>
<p>训练实例示例。已知立体相机对之间的运动限制了深度CNN网络和测距CNN网络。</p>
<h1 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h1><p><img src="/2018/06/19/unsupervised2018-1/framework.png" alt="framework"></p>
<p>训练阶段的网络结构如上图所示  ↑</p>
<p>jointly learning a single view depth ConvNet (CNN<sub>D</sub>) and a visual odometry ConvNet (CNN<sub>V O</sub>) from stereo sequences. </p>
<p>双目视频流克服了单目视频流的尺度模糊问题，并且使得系统能够利用左右（空间上）和前后（时间上）的一致性。</p>
<p>图像重建过程：</p>
<p>I<sub>L,t2</sub> is the reference view while I<sub>L,t1</sub> and I<sub>R,t2</sub> are the live views.</p>
<p>图像重建过程是通过几何变换得到PL, t2在live view上的投影坐标PR, t2和PL, t1。<br><img src="/2018/06/19/unsupervised2018-1/imagerecons.png" alt="imagerecons"></p>
<p>图像重建的损失函数表示为<br><img src="/2018/06/19/unsupervised2018-1/imageloss.png" alt="imageloss"></p>
<p>合成函数由两个可微分的部分组成: <strong>epipolar geometry transformation and warping</strong>.</p>
<p>利用epipolar geometry我们能获得p<sub>L,t2</sub>对应于live views上的投影坐标。live views上的投影坐标表示为<br><img src="/2018/06/19/unsupervised2018-1/coordinate.png" alt="coordinate"></p>
<p>其中D<sub>L,t2(pL,t2)</sub>是在位置p<sub>L,t2</sub>处的深度; T ∈ SE3是由6个参数定义的4x4变换矩阵，6个参数中3个表示角度变换3个表示平移变换。K表示已知的相机内部参数矩阵。</p>
<p><img src="/2018/06/19/unsupervised2018-1/affine.JPG" alt="affine"></p>
<p>(4)和(5)就类似于上面这个式子，是一个仿射变换。上式中xt是输出，xs是输入。有了这个对应关系，根据论文Spatial Transformer Networks中下面这个表达式就可以得到输入图与输出图的对应关系</p>
<p><img src="/2018/06/19/unsupervised2018-1/trans.JPG" alt="trans"></p>
<p>为了提高整体框架的稳健性，提出一种特征重建损失：dense features。</p>
<h2 id="3-1-dense-features"><a href="#3-1-dense-features" class="headerlink" title="3.1 dense features"></a>3.1 dense features</h2><p><a href="https://arxiv.org/abs/1711.05919" target="_blank" rel="noopener">Learning Deeply Supervised Visual Descriptors for Dense Monocular Reconstruction CVPR_2017</a></p>
<h3 id="3-1-1-孔径问题"><a href="#3-1-1-孔径问题" class="headerlink" title="3.1.1 孔径问题"></a>3.1.1 孔径问题</h3><p>何谓孔径问题呢？我们可以先看下面这个动画：</p>
<p><img src="/2018/06/19/unsupervised2018-1/aperture.gif" alt=""> </p>
<p>我们可以看到，中间有一个圆圈，我们透过这个圆圈，会看到有斜线，沿着【右下】的方向移动。然而如果要造成这种视觉效果，却又三种可能。分别是【往下】，【往右】，【往右下】移动。</p>
<p>透过孔径来观察的时候，会有错觉。这就是【区域(local)】和【全域(global)】视觉处理的差别。 </p>
<p>###什么是dense features？<br><img src="/2018/06/19/unsupervised2018-1/network.JPG" alt=""><br>图中得到的f即dense features。<br>Our network consists of five blocks B1-B5 (ref. figure 2), each block consisting of 3 convolutional layers. All convolutional layers have a kernel size of 3x3 and 32 output filters, which we found to be a good middle ground for computational speed and matching accuracy. The first layer of each block has a stride of 2 for 2x downsampling after each subsequent block. <strong>The progressive down sampling rapidly increases the receptive field for a pixel without the need for a deeper network and helps the network capture contextual information which is key for successful matching.</strong> Not having a very deep network also prevents the network from overfitting to a particular dataset, and helps it learn more generic descriptors that are useful for matching even in completely different types of environments (ref. figure 6). <strong>In our network, the maximum potential receptive for a pixel is roughly half the input image resolution</strong> which is significantly larger than in work like [23] which limit the receptive field to 9x9 patches.<br>The downsampling however comes at a cost of reduced precision in the matching. This is because course features, although useful for matching large textureless regions, are too abstract for fine-grain pixel level matching which is required for precise depth estimation. Inspired by the U-Net architecture [16] <strong>we upsample each course prediction and sum it up with the preceding fine predictions (as shown in figure 2) in order to produce our final feature output.</strong> Doing so explicitly embeds local information about a pixel into its contextual representation enabling precise sub-pixel level matching.<br>The final output of our network is therefore a 32 dimen-sional feature vector for each pixel in the image, encoding both contextual and low-level image information. <strong>Each up-sampling operation on the output of each course block is done through a single learnable deconvolution layer with a kernel size of 5x5 and stride of 2.</strong> Making this layer learnable resulted in improved results over a simple bilinear upsampling, showing that in this case it is better to allow the network to learn a more appropriate upsampling method. Fur-thermore, concatenating a bilinearly downsampled version of the input image with the output features from each block before passing them as input into the subsequent courser blocks also improved the matching result. This shows that there is still information present in the low-level features that can complement the abstract ones in order to benefit the pixelwise matching at lower resolutions.</p>
<p>###图中的loss layers</p>
<p>3.1结束。</p>
<p>F<sub>L,t2</sub>，F<sub>L,t1</sub>和F<sub>R,t2</sub>(NYUv2 Feature)分别表示I<sub>L,t2</sub>，I<sub>L,t1</sub>和I<sub>R,t2</sub>的dense features。与image systhesis过程类似，特征重建过程和损失函数表示为<br><img src="/2018/06/19/unsupervised2018-1/featurerecons.png" alt="featurerecons"><br><img src="/2018/06/19/unsupervised2018-1/featureloss.png" alt="featureloss"></p>
<h2 id="3-2-训练误差"><a href="#3-2-训练误差" class="headerlink" title="3.2 训练误差"></a>3.2 训练误差</h2><p><img src="/2018/06/19/unsupervised2018-1/totalloss.png" alt="totalloss"><br>其中L<sub>ds</sub>是为了深度图D更加平滑<br><img src="/2018/06/19/unsupervised2018-1/lds.png" alt="lds"></p>
<h2 id="3-3-网络结构"><a href="#3-3-网络结构" class="headerlink" title="3.3 网络结构"></a>3.3 网络结构</h2><p>深度估计 深度估计卷积网络由两部分组成，encoder and decoder。<br>encoder: ResNet50-1by2（参数大约是ResNet50的1/4）<br>decoder network: the decoder firstly converts the encoder output (1024-channel feature maps) into a single channel feature map using a 1x1 kernel, followed by conventional bilinear upsampling kernels with skip-connections. We use ReLU activation after the last prediction layer to ensure positive prediction comes from the depth ConvNet.<br>视觉测距 视觉测距卷积网络中two concatenated views along the color channels作为输入，输出a 6D vector [u, v]∈se3，接下来转换成a 4x4 transformation matrix。该网络由2次卷积和3次全连接层组成。<br>最后一个全连接层输出6D vector，用来实现reference view到live view的转换。</p>
<h1 id="4-深度估计结果"><a href="#4-深度估计结果" class="headerlink" title="4. 深度估计结果"></a>4. 深度估计结果</h1><p><img src="/2018/06/19/unsupervised2018-1/result.png" alt="result"><br>Full-NYUv2: Stereo + Temporal + NYUv2 Feature</p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/Depth-Estimation/">Depth Estimation</a>
  </div>




<div class="article-share" id="share">

  <div data-url="https://www.lijn.tech/2018/06/19/unsupervised2018-1/" data-title="Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction CVPR_2018 | LI jianan&#39;s Blog" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/06/20/STN/" title="Spatial Transform Networks CVPR_2016">
  <strong>Previous:</strong><br/>
  <span>
  Spatial Transform Networks CVPR_2016</span>
</a>
</div>


<div class="next">
<a href="/2018/06/19/network-configuration/"  title="虚拟机网络配置详解(NAT、桥接、Hostonly)">
 <strong>Next:</strong><br/> 
 <span>虚拟机网络配置详解(NAT、桥接、Hostonly)
</span>
</a>
</div>

</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span> If you are interested in Computer Vision, you can contact </span>
			<span> <a href="https://github.com/JnanLi">me.</a> </span>
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>




<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  </body>
</html>
