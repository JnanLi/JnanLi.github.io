
 <!DOCTYPE HTML>
<html >
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral | LI jianan&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="LI jianan">
    
    <meta name="description" content="本文采用无监督学习的方法来估计深度，基本思路是匹配好左右视图的像素，得到disparity map。根据得到的视差disparity，由d = bf/disparity，算出depth map。本文能实现在35ms内恢复一张图512×256的图只需要25ms（GPU）。               ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/myLogo.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/myLogo.jpg">
    

  
  

    <link rel="stylesheet" href="/css/style.css">
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?d182ed77fc48758bf45a33835ee35745";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

      <script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v1/st.js','_st');

  _st('install','.............Add your swiftype userID...............');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      <div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="LI jianan&#39;s Blog">LI jianan&#39;s Blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
                    <ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					</li>
                <!--<li><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div></li>-->

				</ul>
			</nav>	
</div>
    </header>
    <div id="container" class="clearfix">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/06/19/monodepth/" title="Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral" itemprop="url">Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral</a>
  </h1>
  <p class="article-time">
    <time datetime="2018-06-19T07:20:52.000Z" itemprop="datePublished">2018-06-19</time>
  </p>
</header>
	<div class="article-content">
		
		
		<p>本文采用无监督学习的方法来估计深度，基本思路是匹配好左右视图的像素，得到disparity map。根据得到的视差disparity，由d = bf/disparity，算出depth map。本文能实现在35ms内恢复一张图512×256的图只需要25ms（GPU）。                                                                                                                                                                                                                                                                                   </p>
<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h1><p>我们提出了一种新的训练目标，使得我们的卷积神经网络能够学习执行单个图像深度估计，尽管缺乏ground truth。Exploiting epipolar geometry constraints，我们通过训练我们的网络以产生图像重构损失来生成视差图像。我们表明，单独解决图像重建会导致质量较差的深度图像。为了克服这个问题，我们提出了一种新的训练损失，它强化了相对于左侧和右侧图像产生的差异之间的一致性，与现有方法相比改进了性能和鲁棒性。<br><a href="https://ieeexplore.ieee.org/document/8100182/" target="_blank" rel="noopener">Unsupervised Monocular Depth Estimation with Left-Right Consistency</a></p>
<h1 id="2-重要段落"><a href="#2-重要段落" class="headerlink" title="2. 重要段落"></a>2. 重要段落</h1><p><strong>In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage.</strong></p>
<p><strong>We show that solving for image reconstruction alone re-sults in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency be-tween the disparities produced relative to both the left and right images, leading to improved performance and robustness com-pared to existing approaches.</strong> </p>
<p><strong>Our method is fast and only takes on the order of 35 milliseconds to predict a dense depth map for a 512×256 image on a modern GPU.</strong> </p>
<p><strong>Here we focus on works related to monocular depth estimation, where there is only a single input image, and no assumptions about the scene geometry or types of objects present are made.</strong></p>
<p><strong>The key insight of our method is that we can simultaneously infer both disparities (left-to-right and right-to-left), using only the left input image, and obtain better depths by enforcing them to be consistent with each other.</strong></p>
<p><strong>We want the output disparity map to align with the input left image, meaning the network has to sample from the right image.</strong></p>
<h1 id="3-DispNet"><a href="#3-DispNet" class="headerlink" title="3. DispNet"></a>3. DispNet</h1><p><a href="https://ieeexplore.ieee.org/document/7780807/" target="_blank" rel="noopener">2016-A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</a></p>
<h1 id="3-1-Main-contributions"><a href="#3-1-Main-contributions" class="headerlink" title="3.1 Main contributions:"></a>3.1 Main contributions:</h1><p><strong>Our datasets are the first large-scale datasets to enable training and evaluation of scene flow methods.</strong><br><strong>Present a convolutional network for real-time disparity estimation that provides state-of-the-art results.</strong></p>
<p>By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.</p>
<h1 id="3-2-scene-flow的定义"><a href="#3-2-scene-flow的定义" class="headerlink" title="3.2 scene flow的定义"></a>3.2 scene flow的定义</h1><p>Optimal flow是3D运动在2D平面上的投影。Scene flow被认为是可以从立体视频或者RGBD视频中计算出的潜在3D运动场。假定有t和t+1时间的立体图像对，即有(I<sup>t</sup><sub>L</sub>, I<sup>t</sup><sub>R</sub>, I<sup>t+1</sup><sub>L</sub>, I<sup>t+1</sup><sub>R</sub>)。Scene flow在已知相机内外部参数的情况下，为四张图像中每一个可见点提供其3D位置和3D运动向量。</p>
<p><img src="/2018/06/19/monodepth/Scene.gif" alt="Scene Flow"></p>
<p>红色箭头表示就是来估计scene flow的。</p>
<h1 id="3-3-论文提出了三个数据子集"><a href="#3-3-论文提出了三个数据子集" class="headerlink" title="3.3 论文提出了三个数据子集"></a>3.3 论文提出了三个数据子集</h1><p>The datasets are freely available online：<br><a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/" target="_blank" rel="noopener">传送门</a></p>
<h1 id="3-4-DispNet"><a href="#3-4-DispNet" class="headerlink" title="3.4 DispNet"></a>3.4 DispNet</h1><p>总体上来说，作者沿用了FlowNet的网络框架：每一个网络有可收缩的部分和可扩展的部分，可收缩部分的输出会有一部分长连接到可扩展部分的输入，基本的网络框架可以参见下表：<br><img src="/2018/06/19/monodepth/DispNet.JPG" alt="DispNet"><br>作者通过实验得出一个结论，在可扩展部分加入卷积层可以使得输出视差图更加平滑。<br><strong>Loss weight schedule：</strong><br>训练开始阶段只训练loss6（给loss的权重置1，其他置0），训练过程中，逐渐增加loss1-5的权重并慢慢使低分辨率的loss失效。这让整个网络首先学习到一个粗略的表达，接着以更精细的分辨率去学习。</p>
<h1 id="3-5-本论文的网络结构"><a href="#3-5-本论文的网络结构" class="headerlink" title="3.5 本论文的网络结构"></a>3.5 本论文的网络结构</h1><p><img src="/2018/06/19/monodepth/model.JPG" alt=""></p>
<h1 id="4-Training-Loss"><a href="#4-Training-Loss" class="headerlink" title="4. Training Loss"></a>4. Training Loss</h1><p>We define a loss C<sub>s</sub> <strong>at each output scale s</strong>, forming the total loss as the sum C = C<sub>1</sub> + … + C<sub>4</sub>. Our loss module (below) computes C<sub>s</sub> as combination of three main terms.<br><img src="/2018/06/19/monodepth/lossmodule.gif" alt="lossmodule"><br>Our loss module outputs left and right disparity maps, d<sup>l</sup> and d<sup>r</sup>. The loss combines smoothness, reconstruction, and left-right disparity consistency terms. This same module is repeated at each of the four different output scales. C: Convolution, UC: Up-convolution, S: Bilinear sampling, US: Up-Sampling, SC: Skip connection.<br><img src="/2018/06/19/monodepth/lossall.png" alt="lossall"></p>
<p>分别是：<br><strong>左右视图的灰度匹配部分</strong><br><strong>视差平滑部分（让disparity的分布更加平滑）</strong><br><strong>左右视图的一致性部分(促使左视图中的disparity分布和右视图的disparity图严格相同)</strong></p>
<h2 id="4-1-左右视图的灰度匹配部分"><a href="#4-1-左右视图的灰度匹配部分" class="headerlink" title="4.1 左右视图的灰度匹配部分"></a>4.1 左右视图的灰度匹配部分</h2><p><img src="/2018/06/19/monodepth/ap.png" alt="ap"><br>论文采用L1和SSIM结合作为图像重建损失。</p>
<h3 id="4-1-1-SSIM"><a href="#4-1-1-SSIM" class="headerlink" title="4.1.1 SSIM"></a>4.1.1 SSIM</h3><p><a href="https://ieeexplore.ieee.org/document/1284395/" target="_blank" rel="noopener">Image quality assessment: from error visibility to structural similarity</a><br>SSIM的流程图表示为下图<br><img src="/2018/06/19/monodepth/SSIM.PNG" alt=""><br>我们只需从上图了解到SSIM利用到两张图片局部的亮度(均值)l(x,y)、对比度(方差)c(x,y)和结构信息(相关)s(x,y)。<br>局部范围的均值可以表示亮度信息：<br><img src="/2018/06/19/monodepth/mean.PNG" alt=""><br>方差可以表示对比度信息：<br><img src="/2018/06/19/monodepth/squ.PNG" alt=""><br>l、c和s都需要满足下面三个条件：<br><img src="/2018/06/19/monodepth/condition.PNG" alt=""><br>l、c和s的具体表达式如下：<br><img src="/2018/06/19/monodepth/l.PNG" alt=""><br><img src="/2018/06/19/monodepth/c.PNG" alt=""><br><img src="/2018/06/19/monodepth/s.PNG" alt=""><br>接着得到SSIM在局部图像上的表达式：<br><img src="/2018/06/19/monodepth/ssim_2.PNG" alt=""><br>本文接着利用圆对称高斯加权函数：<br><img src="/2018/06/19/monodepth/gaussian.PNG" alt=""><br>由此得到均值方差和相关的加权表达式：<br><img src="/2018/06/19/monodepth/3.PNG" alt=""><br>最后，全局的Structural Similarity表达式如下：<br><img src="/2018/06/19/monodepth/mssim.png" alt=""><br>(over)</p>
<p>本文使用了一个简化的SSIM用3×3的核，待会儿看看源码它是具体什么表达式。<br>之前看论文留下的问题回答，<br>问：SSIM是对一块区域来说的，只给一个像素点怎么求？<br>答：每一个像素点周围取一个领域。</p>
<h2 id="4-2-视差平滑部分"><a href="#4-2-视差平滑部分" class="headerlink" title="4.2 视差平滑部分"></a>4.2 视差平滑部分</h2><p><img src="/2018/06/19/monodepth/ds.png" alt="ds"></p>
<p>数字图像中，更多的使用差分来近似导数，最简单的梯度近似表达式如下：</p>
<p><img src="/2018/06/19/monodepth/tidu.png" alt="tidu"></p>
<h2 id="4-3-左右视图的一致性部分"><a href="#4-3-左右视图的一致性部分" class="headerlink" title="4.3 左右视图的一致性部分"></a>4.3 左右视图的一致性部分</h2><p><img src="/2018/06/19/monodepth/lr.png" alt="lr"></p>
<h1 id="5-阅读源码"><a href="#5-阅读源码" class="headerlink" title="5. 阅读源码"></a>5. 阅读源码</h1><h2 id="5-1-from-future-import-absolute-import的作用"><a href="#5-1-from-future-import-absolute-import的作用" class="headerlink" title="5.1 from future import absolute_import的作用"></a>5.1 from <strong>future</strong> import absolute_import的作用</h2><p>关于这句<strong>from <strong>future</strong> import absolute_import</strong>的作用:<br>直观地看就是说”加入<strong>绝对引入</strong>这个新特性”。说到<strong>绝对引入</strong>，当然就会想到<strong>相对引入</strong>。那么什么是<strong>相对引入</strong>呢?比如说，你的包结构是这样的: </p>
<p>pkg/<br>pkg/init.py<br>pkg/main.py<br>pkg/string.py</p>
<p>如果你在main.py中写<strong>import string</strong>,那么在Python 2.4或之前, Python会先查找当前目录下有没有string.py, 若找到了，则引入该模块，然后你在main.py中可以直接用string了。如果你是真的想用同目录下的string.py那就好，但是如果你是想用系统自带的标准string.py呢？那其实没有什么好的简洁的方式可以忽略掉同目录的string.py而引入系统自带的标准string.py。这时候你就需要<strong>from <strong>future</strong> import absolute_import</strong>了。这样，你就可以用<strong>import string</strong>来引入系统的标准string.py, 而用<strong>from pkg import string</strong>来引入当前目录下的string.py了</p>
<h2 id="5-2-os-environ-‘TF-CPP-MIN-LOG-LEVEL’-’1’"><a href="#5-2-os-environ-‘TF-CPP-MIN-LOG-LEVEL’-’1’" class="headerlink" title="5.2 os.environ[‘TF_CPP_MIN_LOG_LEVEL’]=’1’"></a>5.2 os.environ[‘TF_CPP_MIN_LOG_LEVEL’]=’1’</h2><p>TF_CPP_MIN_LOG_LEVEL默认值为 0 (显示所有logs)，设置为 1 隐藏INFO logs, 2 额外隐藏WARNING logs, 设置为3所有ERROR logs也不显示。</p>
<h2 id="5-3-tf-app-run"><a href="#5-3-tf-app-run" class="headerlink" title="5.3 tf.app.run()"></a>5.3 tf.app.run()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
<p>我们来看一下源代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""Generic entry point script."""</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> flags</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(main=None)</span>:</span></span><br><span class="line">  f = flags.FLAGS</span><br><span class="line">  f._parse_flags()</span><br><span class="line">  main = main <span class="keyword">or</span> sys.modules[<span class="string">'__main__'</span>].main</span><br><span class="line">  sys.exit(main(sys.argv))</span><br></pre></td></tr></table></figure></p>
<p><strong>处理flag解析，然后执行main函数</strong>，那么flag解析是什么意思呢？诸如这样的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.app.flags.DEFINE_boolean(<span class="string">"self_test"</span>, <span class="keyword">False</span>, <span class="string">"True if running a self test."</span>)</span><br><span class="line">tf.app.flags.DEFINE_boolean(<span class="string">'use_fp16'</span>, <span class="keyword">False</span>,</span><br><span class="line">                            <span class="string">"Use half floats instead of full floats if True."</span>)</span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br></pre></td></tr></table></figure></p>
<h2 id="5-4-Tensorflow-about-Session-and-Graph"><a href="#5-4-Tensorflow-about-Session-and-Graph" class="headerlink" title="5.4 Tensorflow about Session and Graph"></a>5.4 Tensorflow about Session and Graph</h2><h3 id="5-4-1-Session"><a href="#5-4-1-Session" class="headerlink" title="5.4.1 Session"></a>5.4.1 Session</h3><p>Session提供了Operation执行和Tensor求值的环境。如下面所示，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the graph in a session.</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the tensor 'c'.</span></span><br><span class="line"><span class="keyword">print</span> sess.run(c)</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># result: [3., 8.]</span></span><br></pre></td></tr></table></figure></p>
<p>一个Session可能会拥有一些资源，例如Variable或者Queue。当我们不再需要该session的时候，需要将这些资源进行释放。有两种方式，</p>
<ol>
<li>调用session.close()方法；</li>
<li>使用with tf.Session()创建上下文（Context）来执行，当上下文退出时自动释放。</li>
</ol>
<p>上面的例子可以写成,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">print</span> sess.run(c)</span><br></pre></td></tr></table></figure></p>
<p>Session类的构造函数如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Session.__init__(target=<span class="string">''</span>, graph=<span class="keyword">None</span>, config=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p>
<p>如果在创建Session时没有指定Graph，则该Session会加载默认Graph。如果在一个进程中创建了多个Graph，则需要创建不同的Session来加载每个Graph，而每个Graph则可以加载在多个Session中进行计算。<br>执行Operation或者求值Tensor有两种方式：</p>
<ol>
<li><p>调用Session.run()方法： 该方法的定义如下所示，参数fetches便是一个或者多个Operation或者Tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Session.run(fetches, feed_dict=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用Operation.run()或则Tensor.eval()方法： 这两个方法都接收参数session，用于指定在哪个session中计算。但该参数是可选的，默认为None，此时表示在进程默认session中计算。</p>
</li>
</ol>
<p>那如何设置一个Session为默认的Session呢？有两种方式：</p>
<ol>
<li><p>在with语句中定义的Session，在该上下文中便成为默认session；上面的例子可以修改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session():</span><br><span class="line">   <span class="keyword">print</span> c.eval()</span><br></pre></td></tr></table></figure>
</li>
<li><p>在with语句中调用Session.as_default()方法。 上面的例子可以修改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">c = a * b</span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> sess.as_default():</span><br><span class="line">    <span class="keyword">print</span> c.eval()</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="5-4-2-Graph"><a href="#5-4-2-Graph" class="headerlink" title="5.4.2 Graph"></a>5.4.2 Graph</h3><p>Tensorflow中使用tf.Graph类表示可计算的图。图是由操作Operation和张量Tensor来构成，其中Operation表示图的节点（即计算单元），而Tensor则表示图的边（即Operation之间流动的数据单元）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Graph.__init__()</span><br></pre></td></tr></table></figure></p>
<p>上面创建一个空的Graph。</p>
<p>在Tensorflow中，始终存在一个默认的Graph。如果要将Operation添加到默认Graph中，只需要调用定义Operation的函数（例如tf.add()）。如果我们需要定义多个Graph，则需要在with语句中调用Graph.as_default()方法将某个graph设置成默认Graph，于是with语句块中调用的Operation或Tensor将会添加到该Graph中。<br>例如，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">g1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g1.as_default():</span><br><span class="line">    c1 = tf.constant([<span class="number">1.0</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g2:</span><br><span class="line">    c2 = tf.constant([<span class="number">2.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g1) <span class="keyword">as</span> sess1:</span><br><span class="line">    <span class="keyword">print</span> sess1.run(c1)</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=g2) <span class="keyword">as</span> sess2:</span><br><span class="line">    <span class="keyword">print</span> sess2.run(c2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result:</span></span><br><span class="line"><span class="comment"># [ 1.0 ]</span></span><br><span class="line"><span class="comment"># [ 2.0 ]</span></span><br></pre></td></tr></table></figure></p>
<p>如果将上面例子的sess1.run(c1)和sess2.run(c2)中的c1和c2交换一下位置，运行会报错。因为sess1加载的g1中没有c2这个Tensor，同样地，sess2加载的g2中也没有c1这个Tensor。</p>
<h2 id="5-5-tf-train-AdamOptimizer"><a href="#5-5-tf-train-AdamOptimizer" class="headerlink" title="5.5 tf.train.AdamOptimizer"></a>5.5 tf.train.AdamOptimizer</h2><p>目前其他我有看到的Optimizer还有</p>
<ol>
<li>GradientDescentOptimizer</li>
<li>MomentumOptimizer</li>
</ol>
<h2 id="5-6-tf-py-func"><a href="#5-6-tf-py-func" class="headerlink" title="5.6 tf.py_func"></a>5.6 tf.py_func</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.py_func(</span><br><span class="line">    func,</span><br><span class="line">    inp,</span><br><span class="line">    Tout,</span><br><span class="line">    stateful=<span class="keyword">True</span>,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>给一个python自带或者numpy的函数func，把它变成tf的op。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">string_length_tf</span><span class="params">(t)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.py_func(len, [t], [tf.int64])</span><br></pre></td></tr></table></figure></p>
<p>像上面这样就可以输入一个tf对象，输出他的长度。</p>
<h2 id="5-6-tf-string-split"><a href="#5-6-tf-string-split" class="headerlink" title="5.6 tf.string_split"></a>5.6 tf.string_split</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.string_split(</span><br><span class="line">    source,</span><br><span class="line">    delimiter=<span class="string">' '</span>,</span><br><span class="line">    skip_empty=<span class="keyword">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>举例来说，N=2，source[0] = ‘hello world’，source[1] = ‘a b c’，<br>那么<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source = [<span class="string">'hello world'</span>, <span class="string">'a b c'</span>]</span><br><span class="line">st = tf.string_split(source)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(st.values))</span><br></pre></td></tr></table></figure></p>
<p>得到的结果是[b’hello’ b’word’ b’a’ b’b’ b’c’]。</p>
<h2 id="5-7-tf-string-join"><a href="#5-7-tf-string-join" class="headerlink" title="5.7 tf.string_join"></a>5.7 tf.string_join</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a=<span class="string">'aaaa'</span></span><br><span class="line">b=<span class="string">'b/'</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.string_join(b, a)))</span><br></pre></td></tr></table></figure>
<p>2018-07-15 13:36:44.843943: I T:\src\github\tensorflow\tensorflow\core\platform\<br>cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow bi<br>nary was not compiled to use: AVX2<br>Traceback (most recent call last):<br>  File “<stdin>“, line 2, in <module><br>  File “D:\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_string_ops.py”,<br> line 502, in string_join<br>    “‘string_join’ Op, not %r.” % inputs)<br>TypeError: Expected list for ‘inputs’ argument to ‘string_join’ Op, not ‘b/‘.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.string_join([b, a])))</span><br><span class="line"><span class="string">b'b/aaaa'</span></span><br></pre></td></tr></table></figure></module></stdin></p>
<p><strong>Expected list for ‘inputs’ argument to ‘string_join’ Op, not ‘b/‘.</strong>所以输入应该是以一个list的形式</p>
<h2 id="5-8-tf-random-uniform-0-1"><a href="#5-8-tf-random-uniform-0-1" class="headerlink" title="5.8 tf.random_uniform([], 0, 1)"></a>5.8 tf.random_uniform([], 0, 1)</h2><p>第一个参数是shape：<strong>A 1-D integer Tensor or Python array. The shape of the output tensor.</strong></p>
<h2 id="5-9-lambda"><a href="#5-9-lambda" class="headerlink" title="5.9 lambda"></a>5.9 lambda</h2><p>Python中，lambda函数也匿名函数。</p>
<p>lambda语法格式：<br>lambda 变量 : 要执行的语句</p>
<p>当使用tf.cond时，true_fn和false_fn都可以是lambda的形式，或者传入两个def的函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = tf.cond(x &lt; y, <span class="keyword">lambda</span>: tf.add(x, z), <span class="keyword">lambda</span>: tf.square(y))</span><br></pre></td></tr></table></figure></p>
<h2 id="5-10-tf-stack"><a href="#5-10-tf-stack" class="headerlink" title="5.10 tf.stack()"></a>5.10 tf.stack()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.constant([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">c = tf.stack([a,b], axis=<span class="number">0</span>)</span><br><span class="line">d = tf.unstack(c, axis=<span class="number">0</span>)</span><br><span class="line">e = tf.unstack(c, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(c))</span><br><span class="line">    print(sess.run(d))</span><br><span class="line">    print(sess.run(e))</span><br><span class="line">...</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">[array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])]</span><br><span class="line">[array([<span class="number">1</span>, <span class="number">4</span>]), array([<span class="number">2</span>, <span class="number">5</span>]), array([<span class="number">3</span>, <span class="number">6</span>])]</span><br></pre></td></tr></table></figure>
<h2 id="5-11-AttributeError-enter-错误解决"><a href="#5-11-AttributeError-enter-错误解决" class="headerlink" title="5.11 AttributeError enter__错误解决"></a>5.11 AttributeError enter__错误解决</h2><p>tf.Session()错写成tf.Session了。</p>
<h2 id="5-12-Python2和3中的除法运算"><a href="#5-12-Python2和3中的除法运算" class="headerlink" title="5.12 Python2和3中的除法运算"></a>5.12 Python2和3中的除法运算</h2><p>Python 2.x:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1</span> / <span class="number">2</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1.0</span> / <span class="number">2.0</span></span><br><span class="line"><span class="number">0.5</span></span><br></pre></td></tr></table></figure></p>
<p>Python 3.x:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1</span>/<span class="number">2</span></span><br><span class="line"><span class="number">0.5</span></span><br></pre></td></tr></table></figure></p>
<p>而对于//除法，这种除法叫做floor除法，会对除法的结果自动进行一个floor操作，在python 2.x和python 3.x中是一致的。<br>Python 2.x and Python 3.x:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">-1</span> // <span class="number">2</span></span><br><span class="line"><span class="number">-1</span></span><br></pre></td></tr></table></figure></p>
<p>注意的是并不是舍弃小数部分，而是执行floor操作，如果要截取小数部分，那么需要使用math模块的trunc函数。</p>
<h2 id="5-13-tf-variable-scope和tf-name-scope的用法"><a href="#5-13-tf-variable-scope和tf-name-scope的用法" class="headerlink" title="5.13 tf.variable_scope和tf.name_scope的用法"></a>5.13 tf.variable_scope和tf.name_scope的用法</h2><p>tf.variable_scope可以让变量有相同的命名，包括tf.get_variable得到的变量，还有tf.Variable的变量<br>tf.name_scope可以让变量有相同的命名，只是限于tf.Variable的变量<br><strong>reuse: True, None, or tf.AUTO_REUSE; if True, we go into reuse mode for this scope as well as all sub-scopes; if tf.AUTO_REUSE, we create variables if they do not exist, and return them otherwise; if None, we inherit the parent scope’s reuse flag. When eager execution is enabled, this argument is always forced to be tf.AUTO_REUSE.</strong><br>例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf  </span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'V1'</span>):</span><br><span class="line">  a1 = tf.get_variable(name=<span class="string">'a1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">  a2 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>), name=<span class="string">'a2'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'V2'</span>):</span><br><span class="line">  a3 = tf.get_variable(name=<span class="string">'a1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">  a4 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>), name=<span class="string">'a2'</span>)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.initialize_all_variables())</span><br><span class="line">  <span class="keyword">print</span> a1.name</span><br><span class="line">  <span class="keyword">print</span> a2.name</span><br><span class="line">  <span class="keyword">print</span> a3.name</span><br><span class="line">  <span class="keyword">print</span> a4.name</span><br></pre></td></tr></table></figure></p>
<p>输出：<br>V1/a1:0<br>V1/a2:0<br>V2/a1:0<br>V2/a2:0</p>
<p>例子2：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'V1'</span>):</span><br><span class="line">  a1 = tf.get_variable(name=<span class="string">'a1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">  a2 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>), name=<span class="string">'a2'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'V2'</span>):</span><br><span class="line">  a3 = tf.get_variable(name=<span class="string">'a1'</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">  a4 = tf.Variable(tf.random_normal(shape=[<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">0</span>, stddev=<span class="number">1</span>), name=<span class="string">'a2'</span>)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.initialize_all_variables())</span><br><span class="line">  <span class="keyword">print</span> a1.name</span><br><span class="line">  <span class="keyword">print</span> a2.name</span><br><span class="line">  <span class="keyword">print</span> a3.name</span><br><span class="line">  <span class="keyword">print</span> a4.name</span><br></pre></td></tr></table></figure></p>
<p>报错：ValueError: Variable a1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:</p>
<h2 id="5-14-tf-nn-elu"><a href="#5-14-tf-nn-elu" class="headerlink" title="5.14 tf.nn.elu"></a>5.14 tf.nn.elu</h2><p><img src="/2018/06/19/monodepth/elu.png" alt=""></p>
<h2 id="5-15-Python中append和extend函数区别"><a href="#5-15-Python中append和extend函数区别" class="headerlink" title="5.15 Python中append和extend函数区别"></a>5.15 Python中append和extend函数区别</h2><p>append函数直接将object整体当作一个元素追加到列表中，而extend函数则是将可迭代对象中的元素逐个追加到列表中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">lt1=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>]</span><br><span class="line">lt2=[<span class="string">'D'</span>,<span class="string">'E'</span>,<span class="string">'F'</span>]</span><br><span class="line">lt1.append(lt2)<span class="comment">#将lt2整体当作一个元素追加到到lt1中</span></span><br><span class="line">print(lt1)</span><br><span class="line">lt3=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>]</span><br><span class="line">lt2=[<span class="string">'D'</span>,<span class="string">'E'</span>,<span class="string">'F'</span>]</span><br><span class="line">lt3.extend(lt2)<span class="comment">#将lt2中每个元素逐个追加到t3中</span></span><br><span class="line">print(lt3)</span><br><span class="line">   ...:</span><br><span class="line">[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, [<span class="string">'D'</span>, <span class="string">'E'</span>, <span class="string">'F'</span>]]</span><br><span class="line">[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>, <span class="string">'F'</span>]</span><br></pre></td></tr></table></figure></p>
<h2 id="5-16-tf-concat"><a href="#5-16-tf-concat" class="headerlink" title="5.16 tf.concat"></a>5.16 tf.concat</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(</span><br><span class="line">    values,</span><br><span class="line">    axis,</span><br><span class="line">    name=<span class="string">'concat'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Concatenates tensors along one dimension.</strong></p>
<h2 id="5-17-tf-expand-dims"><a href="#5-17-tf-expand-dims" class="headerlink" title="5.17 tf.expand_dims"></a>5.17 tf.expand_dims</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims(</span><br><span class="line">    input,</span><br><span class="line">    axis=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    dim=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape [height, width, channels], you can make it a batch of 1 image with expand_dims(image, 0), which will make the shape [1, height, width, channels].</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 't' is a tensor of shape [2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">0</span>))  <span class="comment"># [1, 2]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line">tf.shape(tf.expand_dims(t, <span class="number">-1</span>))  <span class="comment"># [2, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 't2' is a tensor of shape [2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">0</span>))  <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">2</span>))  <span class="comment"># [2, 3, 1, 5]</span></span><br><span class="line">tf.shape(tf.expand_dims(t2, <span class="number">3</span>))  <span class="comment"># [2, 3, 5, 1]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="5-18-tf-cast"><a href="#5-18-tf-cast" class="headerlink" title="5.18 tf.cast"></a>5.18 tf.cast</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.cast(</span><br><span class="line">    x,</span><br><span class="line">    dtype,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Casts a tensor to a new type.</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1.8</span>, <span class="number">2.2</span>], dtype=tf.float32)</span><br><span class="line">tf.cast(x, tf.int32)  <span class="comment"># [1, 2], dtype=tf.int32</span></span><br></pre></td></tr></table></figure></p>
<h2 id="5-19-tf-meshgrid"><a href="#5-19-tf-meshgrid" class="headerlink" title="5.19 tf.meshgrid"></a>5.19 tf.meshgrid</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.meshgrid(</span><br><span class="line">    *args,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•*args: Tensors with rank 1.</span><br><span class="line">•**kwargs: - indexing: Either &apos;xy&apos; or &apos;ij&apos; (optional, default: &apos;xy&apos;).</span><br></pre></td></tr></table></figure>
<p>meshgrid支持笛卡尔坐标系(直角坐标系, ‘xy’)或是矩阵(‘ij’)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">y = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">X, Y = tf.meshgrid(x, y)</span><br><span class="line"><span class="comment"># X = [[1, 2, 3, 4],</span></span><br><span class="line"><span class="comment">#      [1, 2, 3, 4],</span></span><br><span class="line"><span class="comment">#      [1, 2, 3, 4]]</span></span><br><span class="line"><span class="comment"># Y = [[4, 4, 4, 4],</span></span><br><span class="line"><span class="comment">#      [5, 5, 5, 5],</span></span><br><span class="line"><span class="comment">#      [6, 6, 6, 6]]</span></span><br></pre></td></tr></table></figure></p>
<h2 id="5-20-tf-linspace"><a href="#5-20-tf-linspace" class="headerlink" title="5.20 tf.linspace"></a>5.20 tf.linspace</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.linspace(</span><br><span class="line">    start,</span><br><span class="line">    stop,</span><br><span class="line">    num,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>•start: A Tensor. Must be one of the following types: bfloat16, float32, float64. First entry in the range.<br>•stop: A Tensor. Must have the same type as start. Last entry in the range.<br>•num: A Tensor. Must be one of the following types: int32, int64. Number of values to generate.</strong></p>
<h2 id="5-21-tf-reshape"><a href="#5-21-tf-reshape" class="headerlink" title="5.21 tf.reshape"></a>5.21 tf.reshape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.reshape(</span><br><span class="line">    tensor,</span><br><span class="line">    shape,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor 't' is [[[1, 1], [2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3], [4, 4]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [2, 2, 2]</span></span><br><span class="line">reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2],</span><br><span class="line">                        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor 't' is [[[1, 1, 1],</span></span><br><span class="line"><span class="comment">#                 [2, 2, 2]],</span></span><br><span class="line"><span class="comment">#                [[3, 3, 3],</span></span><br><span class="line"><span class="comment">#                 [4, 4, 4]],</span></span><br><span class="line"><span class="comment">#                [[5, 5, 5],</span></span><br><span class="line"><span class="comment">#                 [6, 6, 6]]]</span></span><br><span class="line"><span class="comment"># tensor 't' has shape [3, 2, 3]</span></span><br><span class="line"><span class="comment"># pass '[-1]' to flatten 't'</span></span><br><span class="line">reshape(t, [-1])    ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]</span><br><span class="line">reshape(t, [1, -1]) ==&gt; [[1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 6 6 6]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># -1 can also be used to infer the shape</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -1 is inferred to be 9:</span></span><br><span class="line">reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3],</span><br><span class="line">                         [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]</span><br></pre></td></tr></table></figure>
<h2 id="5-22-tf-tile"><a href="#5-22-tf-tile" class="headerlink" title="5.22 tf.tile"></a>5.22 tf.tile</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.tile(</span><br><span class="line">    input,</span><br><span class="line">    multiples,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>The output tensor’s i’th dimension has input.dims(i) × multiples[i] elements, and the values of input are replicated multiples[i] times along the ‘i’th dimension.</strong></p>
<p>•input: A Tensor. 1-D or higher.<br>•multiples: <strong>A Tensor</strong>. Must be one of the following types: int32, int64. 1-D. Length must be the same as the number of dimensions in input</p>
<h2 id="5-23-tf-stack"><a href="#5-23-tf-stack" class="headerlink" title="5.23 tf.stack"></a>5.23 tf.stack</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.stack(</span><br><span class="line">    values,</span><br><span class="line">    axis=<span class="number">0</span>,</span><br><span class="line">    name=<span class="string">'stack'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">y = tf.constant([<span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line">z = tf.constant([<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">tf.stack([x, y, z])  <span class="comment"># [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)</span></span><br><span class="line">tf.stack([x, y, z], axis=<span class="number">1</span>)  <span class="comment"># [[1, 2, 3], [4, 5, 6]]</span></span><br></pre></td></tr></table></figure>
<h2 id="5-24-tf-range"><a href="#5-24-tf-range" class="headerlink" title="5.24 tf.range"></a>5.24 tf.range</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.range(limit, delta=<span class="number">1</span>, dtype=<span class="keyword">None</span>, name=<span class="string">'range'</span>)</span><br><span class="line">tf.range(start, limit, delta=<span class="number">1</span>, dtype=<span class="keyword">None</span>, name=<span class="string">'range'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="5-25-tf-gather"><a href="#5-25-tf-gather" class="headerlink" title="5.25 tf.gather"></a>5.25 tf.gather</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.gather(</span><br><span class="line">    params,</span><br><span class="line">    indices,</span><br><span class="line">    validate_indices=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    axis=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/06/19/monodepth/gather.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>temp = tf.range(<span class="number">0</span>,<span class="number">10</span>)*<span class="number">10</span> + tf.constant(<span class="number">1</span>,shape=[<span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>temp = tf.tile(tf.expand_dims(temp, <span class="number">1</span>), [<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>temp2 = tf.gather(temp,[<span class="number">1</span>,<span class="number">5</span>,<span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="meta">... </span>    print(sess.run(temp))</span><br><span class="line"><span class="meta">... </span>    print(sess.run(temp2))</span><br><span class="line">...</span><br><span class="line">[[ <span class="number">1</span>  <span class="number">1</span>]</span><br><span class="line"> [<span class="number">11</span> <span class="number">11</span>]</span><br><span class="line"> [<span class="number">21</span> <span class="number">21</span>]</span><br><span class="line"> [<span class="number">31</span> <span class="number">31</span>]</span><br><span class="line"> [<span class="number">41</span> <span class="number">41</span>]</span><br><span class="line"> [<span class="number">51</span> <span class="number">51</span>]</span><br><span class="line"> [<span class="number">61</span> <span class="number">61</span>]</span><br><span class="line"> [<span class="number">71</span> <span class="number">71</span>]</span><br><span class="line"> [<span class="number">81</span> <span class="number">81</span>]</span><br><span class="line"> [<span class="number">91</span> <span class="number">91</span>]]</span><br><span class="line">[[<span class="number">11</span> <span class="number">11</span>]</span><br><span class="line"> [<span class="number">51</span> <span class="number">51</span>]</span><br><span class="line"> [<span class="number">91</span> <span class="number">91</span>]]</span><br></pre></td></tr></table></figure>
<h2 id="5-26-tf-reduce-mean"><a href="#5-26-tf-reduce-mean" class="headerlink" title="5.26 tf.reduce_mean"></a>5.26 tf.reduce_mean</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_mean(</span><br><span class="line">    input_tensor,</span><br><span class="line">    axis=<span class="keyword">None</span>,</span><br><span class="line">    keepdims=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    reduction_indices=<span class="keyword">None</span>,</span><br><span class="line">    keep_dims=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Computes the mean of elements across dimensions of a tensor. (deprecated arguments)</strong></p>
<h2 id="5-27-tf-train-shuffle-batch"><a href="#5-27-tf-train-shuffle-batch" class="headerlink" title="5.27 tf.train.shuffle_batch"></a>5.27 tf.train.shuffle_batch</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data = np.arange(<span class="number">1</span>, <span class="number">100</span> + <span class="number">1</span>)</span><br><span class="line">data_input = tf.constant(data)</span><br><span class="line"></span><br><span class="line">batch_shuffle = tf.train.shuffle_batch([data_input], enqueue_many=<span class="keyword">True</span>, batch_size=<span class="number">10</span>, capacity=<span class="number">100</span>, min_after_dequeue=<span class="number">10</span>, allow_smaller_final_batch=<span class="keyword">True</span>)</span><br><span class="line">batch_no_shuffle = tf.train.batch([data_input], enqueue_many=<span class="keyword">True</span>, batch_size=<span class="number">10</span>, capacity=<span class="number">100</span>, allow_smaller_final_batch=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    coord = tf.train.Coordinator()</span><br><span class="line">    threads = tf.train.start_queue_runners(coord=coord)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        print(i, sess.run([batch_shuffle, batch_no_shuffle]))</span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br></pre></td></tr></table></figure>
<p><strong>Which yields:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">0 [array([23, 48, 15, 46, 78, 89, 18, 37, 88,  4]), array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]</span><br><span class="line">1 [array([80, 10,  5, 76, 50, 53,  1, 72, 67, 14]), array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20])]</span><br><span class="line">2 [array([11, 85, 56, 21, 86, 12,  9,  7, 24,  1]), array([21, 22, 23, 24, 25, 26, 27, 28, 29, 30])]</span><br><span class="line">3 [array([ 8, 79, 90, 81, 71,  2, 20, 63, 73, 26]), array([31, 32, 33, 34, 35, 36, 37, 38, 39, 40])]</span><br><span class="line">4 [array([84, 82, 33,  6, 39,  6, 25, 19, 19, 34]), array([41, 42, 43, 44, 45, 46, 47, 48, 49, 50])]</span><br><span class="line">5 [array([27, 41, 21, 37, 60, 16, 12, 16, 24, 57]), array([51, 52, 53, 54, 55, 56, 57, 58, 59, 60])]</span><br><span class="line">6 [array([69, 40, 52, 55, 29, 15, 45,  4,  7, 42]), array([61, 62, 63, 64, 65, 66, 67, 68, 69, 70])]</span><br><span class="line">7 [array([61, 30, 53, 95, 22, 33, 10, 34, 41, 13]), array([71, 72, 73, 74, 75, 76, 77, 78, 79, 80])]</span><br><span class="line">8 [array([45, 52, 57, 35, 70, 51,  8, 94, 68, 47]), array([81, 82, 83, 84, 85, 86, 87, 88, 89, 90])]</span><br><span class="line">9 [array([35, 28, 83, 65, 80, 84, 71, 72, 26, 77]), array([ 91,  92,  93,  94,  95,  96,  97,  98,  99, 100])]</span><br></pre></td></tr></table></figure></p>
<h2 id="5-28-tf-add-n"><a href="#5-28-tf-add-n" class="headerlink" title="5.28 tf.add_n"></a>5.28 tf.add_n</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.add_n(</span><br><span class="line">    inputs,</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>inputs: A list of Tensor objects, each with same shape and type.</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf;</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line"> </span><br><span class="line">input1 = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">input2 = tf.Variable(tf.random_uniform([<span class="number">3</span>]))</span><br><span class="line">output = tf.add_n([input1, input2])</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.initialize_all_variables())</span><br><span class="line">  <span class="keyword">print</span> sess.run(input1 + input2)</span><br><span class="line">  <span class="keyword">print</span> sess.run(output)</span><br></pre></td></tr></table></figure></p>
<h2 id="5-28-squeeze"><a href="#5-28-squeeze" class="headerlink" title="5.28 squeeze"></a>5.28 squeeze</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.squeeze(</span><br><span class="line">    input,</span><br><span class="line">    axis=<span class="keyword">None</span>,</span><br><span class="line">    name=<span class="keyword">None</span>,</span><br><span class="line">    squeeze_dims=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed. </strong></p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/深度估计/">深度估计</a>
  </div>




<div class="article-share" id="share">

  <div data-url="https://www.lijn.tech/2018/06/19/monodepth/" data-title="Unsupervised Monocular Depth Estimation with Left-Right Consistency CVPR_2017_oral | LI jianan&#39;s Blog" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2018/06/20/bilinear/" title="Bilinear Interpolation">
  <strong>Previous:</strong><br/>
  <span>
  Bilinear Interpolation</span>
</a>
</div>


<div class="next">
<a href="/2018/06/14/ccbdai/"  title="云计算、大数据和人工智能">
 <strong>Next:</strong><br/> 
 <span>云计算、大数据和人工智能
</span>
</a>
</div>

</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	<div class="copyright">
		<span> If you are interested in Computer Vision, you can contact </span>
			<span> <a href="https://github.com/JnanLi">me.</a> </span>
	<div>
</div></footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  //back to top
  function backToTop(){
    var buttonHTML = $("<a href=\"#top\" id=\"back-top\">" + "<span>Back to Top</span></a>");
    buttonHTML.appendTo($("body"));
    var buttonToTop = $("#back-top");
    // hide #back-top first
    buttonToTop.hide();

    // fade in #back-top
    $(function() {
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                buttonToTop.fadeIn();
            } else {
                buttonToTop.fadeOut();
            }
        });
        // scroll body to 0px on click
        buttonToTop.click(function() {
            $('body,html').animate({
                scrollTop: 0
            }, 800);
            return false;
        });
    });
  }
  backToTop();

  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      ta = $('#toc.toc-aside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
        
    }
  });

  var show = true;
  c.click(function(){
    if(show == true){
        a.addClass('fadeOut').css('display', 'none');
        ta.css('display', 'block').addClass('fadeIn');
        m.addClass('moveMain');  
    }else{
        a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');     
        ta.css('display', 'none'); 
        m.removeClass('moveMain');
        $('#toc.toc-aside').css('display', 'none');
    }
    show = !show;
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{

    $(window).scroll(function(){
      ta.css("top",Math.max(140,240-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>




<script type="text/javascript">
  function footerPosition() {
    var contentHeight = document.documentElement.scrollHeight,
        winHeight = window.innerHeight;
    if(contentHeight <= winHeight) {
      $('footer').addClass('fixed-bottom');
    } else {
      $('footer').removeClass('fixed-bottom');
    }
  }
  footerPosition();
  $(window).resize(footerPosition);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  </body>
</html>
